{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Configuration & Imports\n",
    "\n",
    "This cell contains all necessary library imports and user-configurable parameters. **Modify the configuration variables in this cell to match your environment.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T17:35:06.765031Z",
     "start_time": "2025-10-14T17:35:04.574517Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import timedelta, datetime\n",
    "# DataFrame libraries\n",
    "import pandas as pd\n",
    "import cudf # The core GPU DataFrame library\n",
    "from urllib.parse import quote_plus\n",
    "# Database and Parallelization\n",
    "from sqlalchemy import create_engine, text\n",
    "from dask.distributed import Client, progress\n",
    "from dask_cuda import LocalCUDACluster # For managing GPU workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T17:35:06.782525Z",
     "start_time": "2025-10-14T17:35:06.775353Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Configuration loaded and libraries imported successfully.\n",
      "cuDF has been imported for GPU acceleration.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 1A. Configuration ---\n",
    "username = 'xuhang.liu'\n",
    "password = 'xuhangLIU@HOMES'\n",
    "hostname = 'homes-database.epfl.ch'\n",
    "port = '30767'\n",
    "dbname = 'Shenzhen_Taxi'\n",
    "# Database credentials\n",
    "DB_CONFIG = {\n",
    "    \"user\": username,\n",
    "    \"password\":password,\n",
    "    \"host\": hostname,\n",
    "    \"port\": port,\n",
    "    \"dbname\": dbname\n",
    "}\n",
    "\n",
    "# Create a SQLAlchemy database URL\n",
    "# DB_URL = f\"postgresql+psycopg2://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['dbname']}\"\n",
    "password = quote_plus(password)\n",
    "DB_URL = f\"postgresql://{username}:{password}@{hostname}:{port}/{dbname}\"\n",
    "# Table and File names\n",
    "GPS_TABLE_NAME = \"taxi_data2020_01_01\"\n",
    "OUTPUT_CSV_FILE = \"stay_points_gpu_output.csv\"\n",
    "\n",
    "# Algorithm parameters\n",
    "VELOCITY_THRESHOLD_KMH = 1\n",
    "TIME_THRESHOLD_MINUTES = 10\n",
    "\n",
    "# --- 1B. Display Settings ---\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "print(\"Step 1: Configuration loaded and libraries imported successfully.\")\n",
    "print(\"cuDF has been imported for GPU acceleration.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define Data Scope by Time Range\n",
    "\n",
    "Instead of fetching all taxi IDs, we now define the processing scope by the total time range of the data. We will query the earliest and latest timestamps to determine the entire period to be analyzed. This avoids the bottleneck of fetching millions of unique IDs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T17:35:07.099220Z",
     "start_time": "2025-10-14T17:35:06.833373Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Defining processing scope by time...\n",
      "Connection successful. Querying for max time...\n",
      "Time range has been set: from 2020-01-01 00:00:00 to 2020-01-12 12:08:24\n",
      "\n",
      "Generated 368 time chunks to process in parallel.\n",
      "Example chunks:\n"
     ]
    }
   ],
   "source": [
    "def get_time_range(db_url: str):\n",
    "    \"\"\"Queries the database to find the maximum timestamp and sets a fixed start time of 2020-01-01.\"\"\"\n",
    "    try:\n",
    "        engine = create_engine(db_url)\n",
    "        with engine.connect() as connection:\n",
    "            print(\"Connection successful. Querying for max time...\")\n",
    "            # Query only for the maximum time, as the start time is fixed.\n",
    "            query = text(f\"SELECT MAX(time) as max_time FROM {GPS_TABLE_NAME} WHERE time >= '2020-01-01 00:00:00';\")\n",
    "            result = connection.execute(query).fetchone()\n",
    "\n",
    "            # Hardcode the start time to the beginning of 2020\n",
    "            min_time = datetime(2020, 1, 1, 0, 0, 0)\n",
    "\n",
    "            if result and result.max_time:\n",
    "                max_time = result.max_time\n",
    "                print(f\"Time range has been set: from {min_time} to {max_time}\")\n",
    "                # Ensure the found max_time is not before our hardcoded min_time\n",
    "                if max_time < min_time:\n",
    "                    print(\"Warning: The latest timestamp in the database is before 2020. No data will be processed.\")\n",
    "                    return min_time, min_time # Return a zero-duration range\n",
    "                return min_time, max_time\n",
    "            else:\n",
    "                raise ValueError(\"Could not retrieve a valid max_time from the database.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in Step 2: Could not connect or fetch time range. Please check DB_CONFIG.\")\n",
    "        print(f\"Details: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# --- 2A. Connect and get the total time range ---\n",
    "print(\"Step 2: Defining processing scope by time...\")\n",
    "total_start_time, total_end_time = get_time_range(DB_URL)\n",
    "\n",
    "# --- 2B. Generate time chunks for parallel processing ---\n",
    "time_chunks_all = []\n",
    "if total_start_time and total_end_time:\n",
    "    chunk_duration = timedelta(hours=1)  # Process data in 1-hour chunks\n",
    "    \n",
    "    # CRITICAL: Overlap must be >= TIME_THRESHOLD_MINUTES to catch boundary-spanning events\n",
    "    overlap_duration = timedelta(minutes=TIME_THRESHOLD_MINUTES + 5) # Add a 5-min buffer\n",
    "    \n",
    "    current_start = total_start_time\n",
    "    while current_start < total_end_time:\n",
    "        current_end = current_start + chunk_duration\n",
    "        if current_end > total_end_time:\n",
    "            current_end = total_end_time\n",
    "            \n",
    "        time_chunks_all.append((current_start, current_end))\n",
    "        \n",
    "        if current_end == total_end_time:\n",
    "            break # Reached the end\n",
    "            \n",
    "        current_start = current_end - overlap_duration\n",
    "\n",
    "    print(f\"\\nGenerated {len(time_chunks_all)} time chunks to process in parallel.\")\n",
    "    print(f\"Example chunks:\")\n",
    "\n",
    "else:\n",
    "    print(\"Could not determine time range. Halting execution.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Process Trajectories & Identify Stay Points\n",
    "\n",
    "This is the core of the project. We define the functions for processing a single taxi, ensuring the heavy computation happens on the GPU using `cudf`. We then use `Dask` and `dask-cuda` to distribute these tasks across one or more GPUs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T17:35:07.117068Z",
     "start_time": "2025-10-14T17:35:07.106636Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3A: GPU-based processing logic for time chunks defined.\n"
     ]
    }
   ],
   "source": [
    "# --- 3A. Define the logic for processing a single time chunk on the GPU ---\n",
    "\n",
    "def fetch_chunk_data_cpu(start_time, end_time, db_url: str) -> pd.DataFrame:\n",
    "    \"\"\"Fetches all GPS data within a specific time window into a CPU-based Pandas DataFrame.\"\"\"\n",
    "    query = text(f\"\"\"\n",
    "        SELECT taxiid, time, lon, lat, velocity\n",
    "        FROM {GPS_TABLE_NAME}\n",
    "        WHERE \n",
    "            time >= :start_time AND time < :end_time AND\n",
    "            validity = '1' AND passenger = '0' AND zoneid > 0 AND\n",
    "            velocity < :velocity_threshold\n",
    "    \"\"\")\n",
    "    try:\n",
    "        worker_engine = create_engine(db_url)\n",
    "        with worker_engine.connect() as connection:\n",
    "            return pd.read_sql_query(\n",
    "                query,\n",
    "                connection,\n",
    "                params={\n",
    "                    \"start_time\": start_time,\n",
    "                    \"end_time\": end_time,\n",
    "                    \"velocity_threshold\": VELOCITY_THRESHOLD_KMH\n",
    "                }\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(f\"Worker failed to fetch data for chunk {start_time}-{end_time}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def identify_stay_points_gpu(gdf: cudf.DataFrame) -> cudf.DataFrame:\n",
    "    \"\"\"The core algorithm, operating on a cuDF DataFrame for a single taxi's trajectory.\"\"\"\n",
    "    # This function is now designed to be used with groupby().apply()\n",
    "    if gdf.shape[0] < 2:\n",
    "        return None # Not enough data to process\n",
    "\n",
    "    gdf = gdf.sort_values('time')\n",
    "    gdf['is_staying'] = gdf['velocity'] < VELOCITY_THRESHOLD_KMH\n",
    "    gdf['block_id'] = gdf['is_staying'].diff().ne(0).cumsum()\n",
    "    \n",
    "    staying_blocks = gdf[gdf['is_staying']]\n",
    "    if staying_blocks.empty:\n",
    "        return None\n",
    "    \n",
    "    agg_funcs = {'time': ['min', 'max'], 'lon': ['mean'], 'lat': ['mean']}\n",
    "    stay_points = staying_blocks.groupby('block_id').agg(agg_funcs)\n",
    "    \n",
    "    stay_points.columns = ['start_time', 'end_time', 'lon', 'lat']\n",
    "    stay_points['duration'] = stay_points['end_time'] - stay_points['start_time']\n",
    "    \n",
    "    min_duration = pd.to_timedelta(TIME_THRESHOLD_MINUTES, unit='m')\n",
    "    significant_stays = stay_points[stay_points['duration'] >= min_duration].copy()\n",
    "\n",
    "    if significant_stays.empty:\n",
    "        return None\n",
    "        \n",
    "    significant_stays['duration_minutes'] = significant_stays['duration'].dt.total_seconds() / 60\n",
    "    return significant_stays[['start_time', 'end_time', 'duration_minutes', 'lon', 'lat']].reset_index(drop=True)\n",
    "\n",
    "def process_time_chunk_gpu(time_chunk, db_url: str):\n",
    "    \"\"\"Master function for a single worker: Fetch chunk to CPU -> GPU transfer -> GPU process -> CPU result.\"\"\"\n",
    "    start_time, end_time = time_chunk\n",
    "    try:\n",
    "        # 1. Fetch data for the entire time chunk to CPU memory\n",
    "        chunk_df_cpu = fetch_chunk_data_cpu(start_time, end_time, db_url)\n",
    "        if chunk_df_cpu.empty:\n",
    "            return None\n",
    "\n",
    "        # 2. Transfer data from CPU to GPU\n",
    "        chunk_gdf = cudf.from_pandas(chunk_df_cpu)\n",
    "        \n",
    "        # 3. Process entirely on the GPU using groupby().apply()\n",
    "        # This is the key change: process all taxis in the chunk in one go.\n",
    "        result_gdf = chunk_gdf.groupby('taxiid').apply(identify_stay_points_gpu)\n",
    "\n",
    "        if result_gdf.empty:\n",
    "            return None\n",
    "            \n",
    "        # 4. Transfer result back to CPU for consolidation\n",
    "        result_df_cpu = result_gdf.to_pandas()\n",
    "        return result_df_cpu.reset_index() # taxiid is in the index after groupby\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Worker error on time chunk {start_time}-{end_time}: {e}\")\n",
    "    return None\n",
    "\n",
    "print(\"Step 3A: GPU-based processing logic for time chunks defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T17:35:07.170251Z",
     "start_time": "2025-10-14T17:35:07.166213Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(datetime.datetime(2020, 1, 12, 10, 30), datetime.datetime(2020, 1, 12, 11, 30))\n"
     ]
    }
   ],
   "source": [
    "time_chunks = time_chunks_all[-2]\n",
    "print(time_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T17:35:10.928423Z",
     "start_time": "2025-10-14T17:35:07.220162Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Dask CUDA cluster...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lxhep/miniconda3/envs/rapids-env/lib/python3.10/site-packages/dask_cuda/local_cuda_cluster.py:432: UserWarning: Cannot get CPU affinity for device with index 0, setting default affinity\n",
      "  \"plugins\": worker_plugins(\n",
      "2025-10-14 19:35:08,339 - tornado.application - ERROR - Exception in callback <bound method SystemMonitor.update of <SystemMonitor: cpu: 0 memory: 346 MB fds: 80>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lxhep/miniconda3/envs/rapids-env/lib/python3.10/site-packages/tornado/ioloop.py\", line 945, in _run\n",
      "    val = self.callback()\n",
      "  File \"/home/lxhep/miniconda3/envs/rapids-env/lib/python3.10/site-packages/distributed/system_monitor.py\", line 210, in update\n",
      "    gpu_metrics = nvml.real_time()\n",
      "  File \"/home/lxhep/miniconda3/envs/rapids-env/lib/python3.10/site-packages/distributed/diagnostics/nvml.py\", line 370, in real_time\n",
      "    \"utilization\": _get_utilization(h),\n",
      "  File \"/home/lxhep/miniconda3/envs/rapids-env/lib/python3.10/site-packages/distributed/diagnostics/nvml.py\", line 339, in _get_utilization\n",
      "    return pynvml.nvmlDeviceGetUtilizationRates(h).gpu\n",
      "  File \"/home/lxhep/miniconda3/envs/rapids-env/lib/python3.10/site-packages/pynvml.py\", line 3711, in nvmlDeviceGetUtilizationRates\n",
      "    _nvmlCheckReturn(ret)\n",
      "  File \"/home/lxhep/miniconda3/envs/rapids-env/lib/python3.10/site-packages/pynvml.py\", line 1061, in _nvmlCheckReturn\n",
      "    raise NVMLError(ret)\n",
      "pynvml.NVMLError_Unknown: Unknown Error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask client connected to GPU cluster: http://127.0.0.1:8787/status\n",
      "\n",
      "Submitting 2 time chunks as tasks to Dask workers...\n",
      "\n",
      "Gathering results from workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-14 19:35:10,442 - distributed.worker - ERROR - Compute Failed\n",
      "Key:       process_time_chunk_gpu-fdf710478d9df61643443f414406ffc5\n",
      "State:     executing\n",
      "Task:  <Task 'process_time_chunk_gpu-fdf710478d9df61643443f414406ffc5' process_time_chunk_gpu(..., ...)>\n",
      "Exception: \"TypeError('cannot unpack non-iterable datetime.datetime object')\"\n",
      "Traceback: '  File \"/tmp/ipykernel_44109/4185844314.py\", line 51, in process_time_chunk_gpu\\n'\n",
      "\n",
      "2025-10-14 19:35:10,444 - distributed.worker - ERROR - Compute Failed\n",
      "Key:       process_time_chunk_gpu-6dc062b2f4a867a5689f6d752331a605\n",
      "State:     executing\n",
      "Task:  <Task 'process_time_chunk_gpu-6dc062b2f4a867a5689f6d752331a605' process_time_chunk_gpu(..., ...)>\n",
      "Exception: \"TypeError('cannot unpack non-iterable datetime.datetime object')\"\n",
      "Traceback: '  File \"/tmp/ipykernel_44109/4185844314.py\", line 51, in process_time_chunk_gpu\\n'\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable datetime.datetime object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m     progress(futures)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mGathering results from workers...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDask client and CUDA cluster closed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-env/lib/python3.10/site-packages/distributed/client.py:2555\u001b[0m, in \u001b[0;36mClient.gather\u001b[0;34m(self, futures, errors, direct, asynchronous)\u001b[0m\n\u001b[1;32m   2552\u001b[0m     local_worker \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2554\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[0;32m-> 2555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2556\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gather\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2558\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdirect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_worker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_worker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2561\u001b[0m \u001b[43m        \u001b[49m\u001b[43masynchronous\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43masynchronous\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2562\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 51\u001b[0m, in \u001b[0;36mprocess_time_chunk_gpu\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprocess_time_chunk_gpu\u001b[39m(time_chunk, db_url: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     50\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Master function for a single worker: Fetch chunk to CPU -> GPU transfer -> GPU process -> CPU result.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     start_time, end_time \u001b[38;5;241m=\u001b[39m time_chunk\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;66;03m# 1. Fetch data for the entire time chunk to CPU memory\u001b[39;00m\n\u001b[1;32m     54\u001b[0m         chunk_df_cpu \u001b[38;5;241m=\u001b[39m fetch_chunk_data_cpu(start_time, end_time, db_url)\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable datetime.datetime object"
     ]
    }
   ],
   "source": [
    "# --- 3B. Execute Processing in Parallel on the GPU ---\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "if time_chunks:\n",
    "    # Use dask_cuda to create a cluster of workers, one for each visible GPU\n",
    "    print(\"Initializing Dask CUDA cluster...\")\n",
    "    cluster = LocalCUDACluster()\n",
    "    client = Client(cluster)\n",
    "    print(f\"Dask client connected to GPU cluster: {client.dashboard_link}\")\n",
    "\n",
    "    print(f\"\\nSubmitting {len(time_chunks)} time chunks as tasks to Dask workers...\")\n",
    "    futures = client.map(process_time_chunk_gpu, time_chunks, db_url=DB_URL)\n",
    "    \n",
    "    progress(futures)\n",
    "    \n",
    "    print(\"\\nGathering results from workers...\")\n",
    "    results = client.gather(futures)\n",
    "    \n",
    "\n",
    "    print(\"Dask client and CUDA cluster closed.\")\n",
    "else:\n",
    "    print(\"No time chunks to process. Skipping parallel execution.\")\n",
    "    results = []\n",
    "\n",
    "print(f\"\\nStep 3B: Parallel processing finished in {time.time() - start_time:.2f} seconds.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Consolidate & Store Results\n",
    "\n",
    "The final step is to combine the results (which are now standard Pandas DataFrames) into a single master DataFrame and save it to a CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Step 4: Consolidating and deduplicating results...\")\n",
    "\n",
    "# Filter out any `None` values which may result from empty chunks or errors\n",
    "final_results_list = [res for res in results if res is not None and not res.empty]\n",
    "\n",
    "if not final_results_list:\n",
    "    print(\"No stay points were identified across all time chunks.\")\n",
    "else:\n",
    "    # Concatenate all individual Pandas DataFrames into one\n",
    "    final_df = pd.concat(final_results_list, ignore_index=True)\n",
    "    print(f\"Consolidated {final_df.shape[0]} raw stay points before deduplication.\")\n",
    "    \n",
    "    # --- Deduplication Step ---\n",
    "    # Sort by duration to ensure we keep the longest record of an event\n",
    "    final_df.sort_values('duration_minutes', ascending=False, inplace=True)\n",
    "    \n",
    "    # Drop duplicates based on the unique identifier of a stay point event\n",
    "    final_df.drop_duplicates(subset=['taxiid', 'start_time'], keep='first', inplace=True)\n",
    "    \n",
    "    # Reorder columns for better readability and sort the final output\n",
    "    final_df = final_df[['taxiid', 'start_time', 'end_time', 'duration_minutes', 'lon', 'lat']]\n",
    "    final_df.sort_values(['taxiid', 'start_time'], inplace=True)\n",
    "    \n",
    "    print(f\"Successfully consolidated to {len(final_df)} unique stay points after deduplication.\")\n",
    "    \n",
    "    try:\n",
    "        final_df.to_csv(OUTPUT_CSV_FILE, index=False)\n",
    "        print(f\"Results successfully saved to '{OUTPUT_CSV_FILE}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving results to file: {e}\")\n",
    "\n",
    "    print(\"\\n--- Final Results Sample ---\")\n",
    "    display(final_df.head(20))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
