{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Detect and identify charging events of ride-hailing EV, with dividing waiting and charging time. This code is based on RAPIDS (cuDF & cuPy)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T14:12:49.719685Z",
     "start_time": "2025-11-11T14:12:47.143229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cudf\n",
    "import cupy as cp\n",
    "from sqlalchemy import create_engine, text\n",
    "import sqlalchemy\n",
    "import pandas as pd\n",
    "import os\n",
    "from urllib.parse import quote_plus\n",
    "import subprocess\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "from scipy.spatial import cKDTree\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "import pyarrow.parquet as pq\n",
    "from heapq import heappush, heappop\n",
    "import dask_cudf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import logging, shutil\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Parmaeters"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T14:26:42.672323Z",
     "start_time": "2025-11-10T14:26:42.667555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Thresholds for stay events detection\n",
    "V_THR = 5   # (km/h) Max speed to be considered\n",
    "T_GAP_THR =600      # (s) Max time gap between consecutive points in the same stay\n",
    "D_GAP_THR = 100  # (m) Max distance between points to break a stay segment\n",
    "STAY_THR = 600       # (s) Minimum duration for a sequence of points to be considered a valid stay\n",
    "D_THR_M = 200  #(m) Threshold to consider a stay as a charging event"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T14:26:44.730338Z",
     "start_time": "2025-11-10T14:26:44.726160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Database connection\n",
    "username = 'xuhang.liu'\n",
    "password = 'xuhangLIU@HOMES'\n",
    "hostname = 'homes-database.epfl.ch'\n",
    "port = '30767'\n",
    "dbname = 'Shenzhen_Taxi'\n",
    "password = quote_plus(password)\n",
    "DB_URL = f\"postgresql://{username}:{password}@{hostname}:{port}/{dbname}\""
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Pre-defined fucntions"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T14:26:50.445746Z",
     "start_time": "2025-11-10T14:26:50.440659Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Haversine Distance Function\n",
    "def haversine_distance_gpu(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance in meters between two points\n",
    "    on the earth using cupy for GPU acceleration.\n",
    "    \"\"\"\n",
    "    # cupy operations require float64 for precision in trig functions\n",
    "    lon1_rad = cp.radians(lon1.astype(cp.float64))\n",
    "    lat1_rad = cp.radians(lat1.astype(cp.float64))\n",
    "    lon2_rad = cp.radians(lon2.astype(cp.float64))\n",
    "    lat2_rad = cp.radians(lat2.astype(cp.float64))\n",
    "\n",
    "    # Haversine formula\n",
    "    dlon = lon2_rad - lon1_rad\n",
    "    dlat = lat2_rad - lat1_rad\n",
    "    a = cp.sin(dlat / 2)**2 + cp.cos(lat1_rad) * cp.cos(lat2_rad) * cp.sin(dlon / 2)**2\n",
    "    c = 2 * cp.arcsin(cp.sqrt(a))\n",
    "    # Radius of earth in meters.\n",
    "    R = 6371000\n",
    "    return c * R"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data loading"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save data locally\n",
    "table_name = \"taxi_data2020_01_01\"\n",
    "save_path = os.path.join(os.getcwd(), 'gps_data_01_01_v5.csv')\n",
    "if  not os.path.exists(save_path):\n",
    "    query = f\"\"\"\n",
    "    SELECT\n",
    "        gid, taxiid,\n",
    "        to_char(time, 'YYYY-MM-DD HH24:MI:SS.US') as time,\n",
    "        lon, lat, velocity, angle, passenger, zoneid, validity,\n",
    "        hvalue, \"OSM_edgeid\", \"OSM_distance\"\n",
    "    FROM {table_name}\n",
    "    WHERE  passenger = 0 AND velocity<={V_THR}  AND time >= '2020-01-01'\n",
    "    ORDER BY taxiid, time\n",
    "    \"\"\"\n",
    "\n",
    "    conn = psycopg2.connect(DB_URL)\n",
    "    cur = conn.cursor()\n",
    "    print('data loading...')\n",
    "    with open(save_path, 'w', encoding='utf-8') as f:\n",
    "        cur.copy_expert(f\"COPY ({query}) TO STDOUT WITH CSV HEADER\", f)\n",
    "\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    print('data loaded.')\n",
    "else:\n",
    "    print('data have been saved locally')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-11-10T18:46:00.381699Z"
    }
   },
   "cell_type": "code",
   "source": [
    "table_name = \"taxi_data2020_01_01\"\n",
    "save_path2 = os.path.join(os.getcwd(), 'gps_data_01_01_all.csv')\n",
    "if  not os.path.exists(save_path2):\n",
    "    query = f\"\"\"\n",
    "    SELECT\n",
    "        gid, taxiid,\n",
    "        to_char(time, 'YYYY-MM-DD HH24:MI:SS.US') as time,\n",
    "        lon, lat, velocity, angle, passenger, zoneid, \"OSM_distance\"\n",
    "    FROM {table_name}\n",
    "    WHERE  time >= '2020-01-01'\n",
    "    ORDER BY taxiid, time\n",
    "    \"\"\"\n",
    "\n",
    "    conn = psycopg2.connect(DB_URL)\n",
    "    cur = conn.cursor()\n",
    "    print('data loading...')\n",
    "    with open(save_path2, 'w', encoding='utf-8') as f:\n",
    "        cur.copy_expert(f\"COPY ({query}) TO STDOUT WITH CSV HEADER\", f)\n",
    "\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    print('data loaded.')\n",
    "else:\n",
    "    print('data have been saved locally')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loading...\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Read data as df\n",
    "\n",
    "par_path = 'gps_data_01_01_v5.parquet'\n",
    "if os.path.exists(par_path):\n",
    "    print(\"load parquet file\")\n",
    "    gdf = cudf.read_parquet(par_path)\n",
    "else:\n",
    "    cols_to_keep = ['taxiid', 'time', 'lon', 'lat','velocity', 'zoneid','angle','OSM_edgeid', 'OSM_distance']\n",
    "    data_path = 'gps_data_01_01_v5.csv'\n",
    "    CHUNK_SIZE = 10_000_000  # data size of each reading\n",
    "    list_of_chunks = [] # save data\n",
    "    offset = 0\n",
    "    header_read = False\n",
    "    column_names = None\n",
    "\n",
    "    # Main Reading Loop\n",
    "    while True:\n",
    "        try:\n",
    "            if not header_read:\n",
    "                header_df = cudf.read_csv(data_path, nrows=0)\n",
    "                column_names = header_df.columns.to_list()\n",
    "                header_read = True\n",
    "                offset = 1\n",
    "            # read data\n",
    "            chunk = cudf.read_csv(\n",
    "                data_path,\n",
    "                nrows=CHUNK_SIZE,\n",
    "                skiprows=offset,\n",
    "                header=None,\n",
    "                names=column_names,\n",
    "                dtype={\n",
    "                    'taxiid': 'str',\n",
    "                    'lon': np.float32,\n",
    "                    'lat': np.float32,\n",
    "                    'angle': np.int8,\n",
    "                    'zoneid': np.int8,\n",
    "                    'validity': np.int8\n",
    "                },\n",
    "                parse_dates=['time']\n",
    "            )\n",
    "\n",
    "            if len(chunk) == 0:\n",
    "                print(\"have read all data, concatenating:\")\n",
    "                break\n",
    "\n",
    "            list_of_chunks.append(chunk)\n",
    "            offset += len(chunk)\n",
    "            print(f\" have read {offset-1} data\")\n",
    "        except Exception as e:\n",
    "            print(f\"error in reading: {e}\")\n",
    "            break\n",
    "\n",
    "    # Concatenate\n",
    "    if list_of_chunks:\n",
    "        gdf = cudf.concat(list_of_chunks, ignore_index=True)\n",
    "        print(f\"get dataframe with columns of {len(gdf)}\")\n",
    "        del list_of_chunks\n",
    "    else:\n",
    "        print(\"no data\")\n",
    "        gdf = cudf.DataFrame()\n",
    "    # remove NaN value\n",
    "    gdf = gdf.dropna()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!nvidia-smi",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# convert taxiid to category to save resource\n",
    "gdf['taxiid'] = gdf['taxiid'].astype('category')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# save dataframe\n",
    "par_path = 'gps_data_01_01_v5.parquet'\n",
    "gdf.to_parquet(par_path,engine='pyarrow')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Identify stay events"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calculate gaps within each taxi's trajectory\n",
    "if not gdf.empty:\n",
    "    # Get previous point's time and coordinates\n",
    "    gdf['prev_time'] = gdf.groupby('taxiid')['time'].shift()\n",
    "    gdf['prev_lon'] = gdf.groupby('taxiid')['lon'].shift().fillna(0)\n",
    "    gdf['prev_lat'] = gdf.groupby('taxiid')['lat'].shift().fillna(0)\n",
    "\n",
    "    # Calculate time gap between two consecutive points(unit:second)\n",
    "    gdf['dt'] = (gdf['time'] - gdf['prev_time']).dt.seconds.fillna(99999)\n",
    "\n",
    "    # Calculate distance between two consecutive points(unit:second)\n",
    "    gdf['dd'] = haversine_distance_gpu(gdf['lon'], gdf['lat'], gdf['prev_lon'], gdf['prev_lat'])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if not gdf.empty:\n",
    "    # Flag the start of a stay, the first point always correspond a stay segment\n",
    "    stay_flags = (gdf['dt'] > T_GAP_THR) | (gdf['dd'] > D_GAP_THR) | (gdf['prev_time'].isnull())\n",
    "\n",
    "    # Assign a unique ID to each stay and convert the boolean series to a cupy array for cumsum\n",
    "    gdf['stay_tag'] = cp.asarray(stay_flags).astype(cp.int32).cumsum()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!nvidia-smi",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "gdf = gdf[['taxiid', 'stay_tag', 'time', 'lon', 'lat']]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Aggregate by stay tags to find stays\n",
    "if not gdf.empty:\n",
    "    #Group by tag and taxi\n",
    "    agg_stays = gdf.groupby(['taxiid', 'stay_tag']).agg(\n",
    "        start_time=('time', 'min'),\n",
    "        end_time=('time', 'max'),\n",
    "        stay_lon=('lon', 'mean'),\n",
    "        stay_lat=('lat', 'mean'),\n",
    "        points=('taxiid', 'count')\n",
    "    ).reset_index()\n",
    "\n",
    "    #  Calculate duration and filter for valid stays\n",
    "    agg_stays['duration_s'] = (agg_stays['end_time'] - agg_stays['start_time']).dt.seconds\n",
    "    final_stays = agg_stays[agg_stays['duration_s'] >= STAY_THR].copy()\n",
    "    final_stays = final_stays.sort_values(['taxiid', 'start_time'])\n",
    "    print(f\"Found {len(final_stays)} valid stay events.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Matching with stations\n"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# station and piles data\n",
    "STA_CSV_PATH = 'station_information.csv'\n",
    "stations_df = pd.read_csv(STA_CSV_PATH,dtype={'station_id': np.int32})\n",
    "PILE_CSV_PATH = 'pile_rated_power.csv'\n",
    "piles_df = pd.read_csv(PILE_CSV_PATH,dtype={'station_id': np.int32,'pile_id': np.int32})\n",
    "# filter piles (0 < power <= max_power) ---\n",
    "max_power =60\n",
    "valid_piles_df = piles_df[(piles_df['power'] > 0) & (piles_df['power'] <= max_power)].copy()\n",
    "valid_piles_df['low_power_piles'] = (valid_piles_df['power'] < 10).astype(int)\n",
    "valid_piles_df['high_power_piles'] = (valid_piles_df['power'] >= 10).astype(int)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "powers = valid_piles_df['power'].astype('float32')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "bins = np.arange(0, powers.max() + 5, 10)\n",
    "sns.histplot(\n",
    "    powers,\n",
    "    bins=bins,\n",
    "    color='#377eb8',\n",
    "    edgecolor='black',\n",
    "    alpha=0.8,\n",
    "    ax=ax,\n",
    "    stat='percent'\n",
    ")\n",
    "ax.yaxis.set_major_formatter(FormatStrFormatter('%.0f%%'))\n",
    "ax.set_yticks(np.arange(0, ax.get_ylim()[1]+1, 10))\n",
    "ax.set_xlabel('Charger Power (kW)')\n",
    "ax.set_ylabel('percent')\n",
    "ax.set_title('Distribution of Charger Rated Power')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "valid_piles_df.head()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "stat_pile_df = valid_piles_df.groupby('station_id').agg(\n",
    "        num_LP_piles=('low_power_piles', 'sum'),\n",
    "        num_HP_piles=('high_power_piles', 'sum'),\n",
    "        num_dist_powers=('power', 'nunique'),\n",
    "        num_piles =('power','count'),\n",
    "        max_power =('power','max'),\n",
    "        min_power =('power','min'),\n",
    "        avg_power =('power','mean')\n",
    "    ).reset_index()\n",
    "stat_pile_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test = stat_pile_df['num_piles'].isna().any()\n",
    "test"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# stations and piles info vis\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "stat_pile_df['pile_conf'] = np.select(\n",
    "    condlist=[\n",
    "        (stat_pile_df['num_LP_piles'] > 0) & (stat_pile_df['num_HP_piles'] == 0),\n",
    "        (stat_pile_df['num_HP_piles'] > 0) & (stat_pile_df['num_LP_piles'] == 0),\n",
    "        (stat_pile_df['num_LP_piles'] > 0) & (stat_pile_df['num_HP_piles'] > 0)\n",
    "    ],\n",
    "    choicelist=['Slow charging', 'Fast charging', 'Hybrid'],\n",
    "    default='Other'\n",
    ")\n",
    "\n",
    "type_counts = stat_pile_df['pile_conf'].value_counts().reindex(\n",
    "    ['Slow charging', 'Fast charging', 'Hybrid'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# pile configuration vis\n",
    "colors = ['#4daf4a', '#377eb8', '#ff7f00']   # Slow, Fast, Mixed\n",
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "wedges, texts, autotexts = ax.pie(\n",
    "    type_counts.values,\n",
    "    labels=type_counts.index,\n",
    "    autopct='%1.1f%%',\n",
    "    startangle=10,\n",
    "    counterclock=False,\n",
    "    colors=colors,\n",
    "    pctdistance=0.7,\n",
    "    textprops={'fontsize': 10}\n",
    ")\n",
    "centre_circle = plt.Circle((0,0), 0.45, fc='white')\n",
    "for text in texts:\n",
    "    text.set_color('black')\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color('black')\n",
    "\n",
    "slow_charging_label_index = 0\n",
    "if 'Slow charging' in type_counts.index:\n",
    "    slow_charging_label_index = list(type_counts.index).index('Slow charging')\n",
    "\n",
    "slow_label = texts[slow_charging_label_index]\n",
    "slow_autopct = autotexts[slow_charging_label_index]\n",
    "slow_label.set_position((-0.3, -1.05))\n",
    "slow_label.set_ha('center')\n",
    "\n",
    "fig.gca().add_artist(centre_circle)\n",
    "#ax.set_title('Station piles configuration', fontsize=14)\n",
    "fig.patch.set_alpha(1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# slow charging proportion in hybrid stations\n",
    "mixed_df = stat_pile_df[stat_pile_df['pile_conf'] == 'Hybrid'].copy()\n",
    "mixed_df['share_LP'] = (\n",
    "    mixed_df['num_LP_piles'] / (mixed_df['num_LP_piles'] + mixed_df['num_HP_piles'])\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(6,4))\n",
    "# KDE\n",
    "sns.kdeplot(\n",
    "    data=mixed_df,\n",
    "    x='share_LP',\n",
    "    fill=True,\n",
    "    color='#ff7f00',\n",
    "    ax=axes\n",
    ")\n",
    "axes.set_xlabel('Proportion of low-power piles')\n",
    "axes.set_xlim(0,1)\n",
    "axes.grid(False)\n",
    "axes.xaxis.set_major_formatter(FormatStrFormatter('%g'))\n",
    "#axes.set_title('KDE of Slow-Charger Share')\n",
    "#sns.despine(offset=10, trim=True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "mixed_df = stat_pile_df[stat_pile_df['pile_conf'] == 'Hybrid'].copy()\n",
    "mixed_df['share_LP'] = (\n",
    "    mixed_df['num_LP_piles'] /\n",
    "    (mixed_df['num_LP_piles'] + mixed_df['num_HP_piles'])\n",
    ")\n",
    "total_mixed = len(mixed_df)\n",
    "\n",
    "bins = np.linspace(0,1, 11)         # 0–1 分成 10 段\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "\n",
    "counts, bin_edges, patches = ax.hist(\n",
    "    mixed_df['share_LP'],\n",
    "    bins=bins,\n",
    "    weights=np.ones(total_mixed) / total_mixed,\n",
    "    color='pink',\n",
    "    edgecolor='black',\n",
    "    alpha=0.9\n",
    ")\n",
    "\n",
    "ax.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "ax.xaxis.set_major_formatter(FormatStrFormatter('%g'))\n",
    "ax.set_xlabel('Proportion of low-power piles')\n",
    "ax.set_ylabel('Percentage in hybrid stations')\n",
    "#ax.set_title('Slow-Charger Share among Mixed Stations')\n",
    "ax.grid(False)\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_xticks(np.arange(0, 1.1, 0.1))\n",
    "\n",
    "for frac, left, right in zip(counts, bin_edges[:-1], bin_edges[1:]):\n",
    "    if frac == 0:\n",
    "        continue\n",
    "    x_pos = (left + right) / 2\n",
    "    pct_text = f'{frac*100:.0f}%'\n",
    "    ax.text(\n",
    "        x_pos, frac + 0.003,\n",
    "        pct_text,\n",
    "        ha='center', va='bottom',\n",
    "        fontsize=10,\n",
    "    )\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# number pile power types\n",
    "power_diversity_counts = (\n",
    "    stat_pile_df['num_dist_powers']\n",
    "      .value_counts()\n",
    "      .sort_index()\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "labels = [f'{k} types' for k in power_diversity_counts.index]\n",
    "plt.pie(\n",
    "    power_diversity_counts.values,\n",
    "    labels=labels,\n",
    "    autopct='%1.1f%%',\n",
    "    startangle=90,\n",
    "    radius=1.1,\n",
    "    counterclock=False,\n",
    "    textprops={'fontsize': 10}\n",
    ")\n",
    "#plt.title('Distribution of piles power across stations',fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T16:22:51.142345Z",
     "start_time": "2025-11-03T16:22:51.134679Z"
    }
   },
   "cell_type": "markdown",
   "source": "# most of charging piles about 95% are slow-charging(<10KW), and most stations only have slow-charing piles, evne the hybrid stations, most(>85%) of them have slow-charging piles accounting for more than 50%.  The conclusion is slow-charging is dominant"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#  Refine Station data\n",
    "try:\n",
    "    original_station_id_dtype = stations_df['station_id'].dtype\n",
    "    core_station_columns = ['station_id', 'longitude', 'latitude', 'TAZID']\n",
    "    core_station_columns = [col for col in core_station_columns if col in stations_df.columns]\n",
    "\n",
    "    # stat_pile_df = stat_pile_stats.groupby('station_id').agg(\n",
    "    #     low_power_piles=('low_power_piles', 'sum'),\n",
    "    #     high_power_piles=('high_power_piles', 'sum')\n",
    "    # ).reset_index()\n",
    "    temp_stations_df = stations_df.copy()\n",
    "    temp_counts_df = stat_pile_df.copy()\n",
    "\n",
    "    if temp_stations_df['station_id'].dtype != temp_counts_df['station_id'].dtype:\n",
    "        temp_stations_df['station_id'] = temp_stations_df['station_id'].astype(int)\n",
    "        temp_counts_df['station_id'] = temp_counts_df['station_id'].astype(int)\n",
    "\n",
    "    stations_df_cleaned = temp_stations_df[core_station_columns]\n",
    "\n",
    "    # match station id\n",
    "    stations_df_updated = pd.merge(\n",
    "        stations_df_cleaned,\n",
    "        temp_counts_df,\n",
    "        on='station_id',\n",
    "        how='right'\n",
    "    )\n",
    "\n",
    "    # # update piles number\n",
    "    # stations_df_updated['charge_count'] = stations_df_updated['low_power_piles'] + stations_df_updated['high_power_piles']\n",
    "    # # only keep stations with at least 1 valid pile\n",
    "    # stations_df_updated = stations_df_updated[ stations_df_updated['charge_count']>0]\n",
    "\n",
    "    stations_df = stations_df_updated\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"no {PILE_CSV_PATH}\")\n",
    "except Exception as e:\n",
    "    print(f\"error: {e}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# --- 2. Build cKDTree from Station Coordinates (on CPU) ---\n",
    "# This assumes both GPS and station data use the WGS-84 coordinate system.\n",
    "station_coords = stations_df[['longitude', 'latitude']].to_numpy()\n",
    "station_tree = cKDTree(station_coords)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#  match stay events with stations\n",
    "if not final_stays.empty:\n",
    "\n",
    "    # Transfer stay-point coordinates from GPU to CPU\n",
    "    stay_coords = final_stays[['stay_lon', 'stay_lat']].to_numpy()\n",
    "    #Perform the nearest neighbor query using the cKDTree\n",
    "    distances_deg, indices = station_tree.query(stay_coords, k=1)\n",
    "\n",
    "    #  Convert distance from degrees to approximate meters.\n",
    "    distances_m = distances_deg * 111320\n",
    "\n",
    "    # Transfer the numpy arrays from CPU back to GPU\n",
    "    final_stays['distance_to_station'] = distances_m\n",
    "\n",
    "    # Map the indices from the query result back to the original station_id\n",
    "    station_id_lookup = stations_df['station_id'].to_numpy()\n",
    "    nearest_station_ids = station_id_lookup[indices]\n",
    "    final_stays['nearest_station_id'] = cudf.Series(nearest_station_ids, index=final_stays.index)\n",
    "\n",
    "    # Filter for charging events within distance threshold ---\n",
    "    charging_events = final_stays[final_stays['distance_to_station'] <= D_THR_M].copy()\n",
    "    charging_events = charging_events.sort_values(['taxiid', 'start_time'])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T14:15:36.271546Z",
     "start_time": "2025-11-11T14:15:36.263774Z"
    }
   },
   "cell_type": "code",
   "execution_count": 4,
   "source": "os.getcwd()",
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T14:42:20.683301Z",
     "start_time": "2025-11-11T14:42:20.676628Z"
    }
   },
   "cell_type": "code",
   "source": "mypath=\"/home/lxhep/etaxi/data/taxi_trajectories.parquet/taxiid=UUUB0C0M7\"",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T14:42:21.225406Z",
     "start_time": "2025-11-11T14:42:21.205212Z"
    }
   },
   "cell_type": "code",
   "source": "mydata = pd.read_parquet(mypath)",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T14:42:22.287413Z",
     "start_time": "2025-11-11T14:42:22.278969Z"
    }
   },
   "cell_type": "code",
   "source": "mydata.head(10)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                 time         lon        lat  velocity  passenger\n",
       "0 2020-01-01 00:00:43  114.098412  22.543447         0          1\n",
       "1 2020-01-01 00:01:28  114.098412  22.543447         0          1\n",
       "2 2020-01-01 00:01:58  114.098618  22.543428         0          1\n",
       "3 2020-01-01 00:02:13  114.098618  22.543428         0          1\n",
       "4 2020-01-01 00:03:13  114.098618  22.543428        13          1\n",
       "5 2020-01-01 00:04:13  114.098930  22.543468         0          1\n",
       "6 2020-01-01 00:04:58  114.098930  22.543468         0          1\n",
       "7 2020-01-01 00:05:15  114.099663  22.543573        17          1\n",
       "8 2020-01-01 00:06:00  114.099579  22.546421        23          1\n",
       "9 2020-01-01 00:06:43  114.099556  22.548313        42          1"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>velocity</th>\n",
       "      <th>passenger</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01 00:00:43</td>\n",
       "      <td>114.098412</td>\n",
       "      <td>22.543447</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-01 00:01:28</td>\n",
       "      <td>114.098412</td>\n",
       "      <td>22.543447</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-01 00:01:58</td>\n",
       "      <td>114.098618</td>\n",
       "      <td>22.543428</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-01 00:02:13</td>\n",
       "      <td>114.098618</td>\n",
       "      <td>22.543428</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-01 00:03:13</td>\n",
       "      <td>114.098618</td>\n",
       "      <td>22.543428</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020-01-01 00:04:13</td>\n",
       "      <td>114.098930</td>\n",
       "      <td>22.543468</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2020-01-01 00:04:58</td>\n",
       "      <td>114.098930</td>\n",
       "      <td>22.543468</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2020-01-01 00:05:15</td>\n",
       "      <td>114.099663</td>\n",
       "      <td>22.543573</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2020-01-01 00:06:00</td>\n",
       "      <td>114.099579</td>\n",
       "      <td>22.546421</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2020-01-01 00:06:43</td>\n",
       "      <td>114.099556</td>\n",
       "      <td>22.548313</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T10:11:54.544417Z",
     "start_time": "2025-11-11T10:11:54.539366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "stay_par_path = 'stay_01_01_v5.parquet'\n",
    "char_par_path = 'char_01_01_v5.parquet'\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T10:11:48.508382Z",
     "start_time": "2025-11-11T10:11:48.486386Z"
    }
   },
   "cell_type": "code",
   "source": [
    "final_stays.to_parquet(stay_par_path,engine='pyarrow')\n",
    "charging_events.to_parquet(char_par_path,engine='pyarrow')"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_stays' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mfinal_stays\u001B[49m\u001B[38;5;241m.\u001B[39mto_parquet(stay_par_path,engine\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpyarrow\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      2\u001B[0m charging_events\u001B[38;5;241m.\u001B[39mto_parquet(char_par_path,engine\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpyarrow\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'final_stays' is not defined"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T10:11:59.796670Z",
     "start_time": "2025-11-11T10:11:59.324861Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if os.path.exists(stay_par_path):\n",
    "    final_stays = pd.read_parquet(stay_par_path)\n",
    "    print(f'stay parquet loaded')\n",
    "if os.path.exists(char_par_path):\n",
    "    charging_events = pd.read_parquet(char_par_path)\n",
    "    print(f'char parquet loaded')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stay parquet loaded\n",
      "char parquet loaded\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T10:26:19.533306Z",
     "start_time": "2025-11-11T10:26:19.526765Z"
    }
   },
   "cell_type": "code",
   "source": "unique_ids= charging_events['taxiid'].unique().tolist()",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T10:26:56.839435Z",
     "start_time": "2025-11-11T10:26:56.833457Z"
    }
   },
   "cell_type": "code",
   "source": "len(unique_ids)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19496"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T10:27:00.866188Z",
     "start_time": "2025-11-11T10:27:00.853310Z"
    }
   },
   "cell_type": "code",
   "source": "np.save('unique_ids.npy', np.array(unique_ids))",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Queue simulation and waiting time calculation"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def que_station(df):\n",
    "    \"\"\"\n",
    "    Simulates the queue for a single station. The input 'df' is for one station and sorted by 'start_time'.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        print('input df is empty')\n",
    "        return None\n",
    "\n",
    "    k = int(df['num_piles'].iloc[0])\n",
    "    busy_chargers_heap = []  # A min-heap storing the end_time of cars currently charging\n",
    "    results = []\n",
    "\n",
    "    for row in df.itertuples():\n",
    "        arrival_time = row.start_time\n",
    "        departure_time = row.end_time\n",
    "\n",
    "        # Remove chargers that have become free before the current car arrives\n",
    "        while busy_chargers_heap and busy_chargers_heap[0] <= arrival_time:\n",
    "            heappop(busy_chargers_heap)\n",
    "\n",
    "        # Check for available chargers\n",
    "        if len(busy_chargers_heap) < k:\n",
    "            # There is a free charger, no waiting time\n",
    "            wait_seconds = 0\n",
    "            charge_start_time = arrival_time\n",
    "        else:\n",
    "            # All chargers are busy, find the earliest time a charger becomes free.\n",
    "            earliest_free_time = heappop(busy_chargers_heap)\n",
    "            wait_seconds = (earliest_free_time - arrival_time).total_seconds()\n",
    "            charge_start_time = earliest_free_time\n",
    "\n",
    "        # Determine if the driver gave up\n",
    "        stay_duration = (departure_time - arrival_time).total_seconds()\n",
    "        gave_up = (wait_seconds >= stay_duration)\n",
    "\n",
    "        # Calculate actual charge duration and update charger status\n",
    "        if gave_up:\n",
    "            actual_charge_seconds = 0\n",
    "            # If the car gave up, the charger it was waiting for is free again at its scheduled time\n",
    "            if 'earliest_free_time' in locals():\n",
    "                 heappush(busy_chargers_heap, earliest_free_time)\n",
    "        else:\n",
    "            actual_charge_seconds = (departure_time - charge_start_time).total_seconds()\n",
    "            # This car occupies a charger until its departure time\n",
    "            heappush(busy_chargers_heap, departure_time)\n",
    "\n",
    "        results.append({\n",
    "            'station_id': row.nearest_station_id,\n",
    "            'taxiid': row.taxiid,\n",
    "            'arrive_time': arrival_time,\n",
    "            'leave_time': departure_time,\n",
    "            'wait_dur': wait_seconds,\n",
    "            'giveup': gave_up,\n",
    "            'charge_dur': actual_charge_seconds\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Queue simulation\n",
    "char_que_df = pd.DataFrame()\n",
    "if 'charging_events' in locals() and not charging_events.empty:\n",
    "\n",
    "    # Select necessary columns and convert to pandas\n",
    "    queue_input_df = charging_events[[\n",
    "        'nearest_station_id', 'taxiid', 'start_time', 'end_time']].to_pandas()\n",
    "\n",
    "    # Merge with station info\n",
    "    queue_input_df = queue_input_df.merge(\n",
    "        stations_df[['station_id', 'num_piles']],\n",
    "        left_on='nearest_station_id',\n",
    "        right_on='station_id',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # # --- 2. Save to Parquet for efficient batch processing ---\n",
    "    # QUEUE_INPUT_PARQUET = 'charging_events_for_queue_sim.parquet'\n",
    "    # queue_input_df.to_parquet(QUEUE_INPUT_PARQUET)\n",
    "    DST_PARQUET = 'char_que_analysis.parquet'\n",
    "    # --- Main Batch Processing Loop ---\n",
    "    writer = None\n",
    "\n",
    "    queue_input_df = queue_input_df.sort_values(['nearest_station_id', 'start_time'], kind='mergesort')\n",
    "    all_station_results = [\n",
    "            que_station(group)\n",
    "            for _, group in queue_input_df.groupby('nearest_station_id', sort=False)]\n",
    "    # Concatenate results and write to the output Parquet file\n",
    "    if all_station_results:\n",
    "        char_que_df = pd.concat(all_station_results, ignore_index=True)\n",
    "        table = pa.Table.from_pandas(char_que_df)\n",
    "        if writer is None:\n",
    "            writer = pq.ParquetWriter(DST_PARQUET, table.schema)\n",
    "        writer.write_table(table)\n",
    "    if writer:\n",
    "        writer.close()\n",
    "    print(f\" Queue simulation finished. Results saved to '{DST_PARQUET}'.\")\n",
    "else:\n",
    "    print(\"'charging_events' DataFrame not found or is empty. Cannot proceed with queue simulation.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# basic data property\n",
    "avg_wait_time = char_que_df['wait_dur'].mean()\n",
    "giveup_rate = char_que_df['giveup'].mean()\n",
    "num_veh = char_que_df['taxiid'].nunique()\n",
    "print(f\"\\nAverage waiting time: {avg_wait_time:.2f} seconds\")\n",
    "print(f\"Give-up rate: {giveup_rate:.2%}\")\n",
    "print(f'{len(charging_events)} charging events from {num_veh} taxis')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Taxi trajectory data"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T16:05:33.609053Z",
     "start_time": "2025-11-10T16:05:33.599618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "TMP_DIR   = 'tmp_tracks'\n",
    "DS_ENERGY = 'energy_gap.parquet'\n",
    "DS_PICK   = 'post_charge_pickup.parquet'\n",
    "os.makedirs(TMP_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "username = 'xuhang.liu'\n",
    "password = 'xuhangLIU@HOMES'\n",
    "hostname = 'homes-database.epfl.ch'\n",
    "port = '30767'\n",
    "dbname = 'Shenzhen_Taxi'\n",
    "password = quote_plus(password)\n",
    "DB_URL = f\"postgresql://{username}:{password}@{hostname}:{port}/{dbname}\"\n",
    "table_name = \"taxi_data2020_01_01\"\n",
    "\n",
    "BATCH_DL_THREADS = 8\n",
    "V_THR = 5\n",
    "CAP_KWH = 57\n",
    "AVG_SPEED_KMH, CONS_KWH_PER_KM = 25, 0.18\n",
    "IDLE_KW = 0\n",
    "MAX_PICK_MIN = 30\n",
    "\n",
    "# connection to sql\n",
    "def sql_engine(uri=DB_URL):\n",
    "    return create_engine(uri, pool_pre_ping=True, pool_size=BATCH_DL_THREADS)\n",
    "\n",
    "\n",
    "# download taxi trajectory data\n",
    "def download_one(tid: str, dst: str, engine, tab_name: str):\n",
    "    if os.path.exists(dst):\n",
    "        return dst\n",
    "    sql_copy = f\"\"\"\n",
    "        COPY (\n",
    "          SELECT taxiid,time,lon,lat,velocity,passenger\n",
    "            FROM {tab_name}\n",
    "           WHERE taxiid = '{tid}'\n",
    "             AND time >= '2020-01-01'\n",
    "           ORDER BY time\n",
    "        ) TO STDOUT WITH CSV\n",
    "    \"\"\"\n",
    "    tmp_csv = dst + '.csv'\n",
    "    with engine.begin() as sa_conn:\n",
    "        pg_conn = sa_conn.connection\n",
    "        with open(tmp_csv, 'w', encoding='utf-8') as f, pg_conn.cursor() as cur:\n",
    "            cur.copy_expert(sql_copy, f)\n",
    "\n",
    "    cudf.read_csv(\n",
    "        tmp_csv,\n",
    "        names=['taxiid','time','lon','lat','velocity','passenger'],\n",
    "        dtype={'taxiid':'str','lon':'f4','lat':'f4','velocity':'f4','passenger':'i1'},\n",
    "        parse_dates=['time']\n",
    "    ).to_parquet(dst)\n",
    "    os.remove(tmp_csv)\n",
    "    return dst"
   ],
   "outputs": [],
   "execution_count": 79
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Energy consumption"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T18:12:46.648939Z",
     "start_time": "2025-11-10T18:12:46.641325Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def energy_between_charges(track: cudf.DataFrame, ce: cudf.DataFrame) -> cudf.DataFrame:\n",
    "    track = track.sort_values('time')\n",
    "\n",
    "    # ① 连续差分并填 NA，确保无掩码\n",
    "    track['lon_prev'] = track['lon'].shift().fillna(track['lon'])\n",
    "    track['lat_prev'] = track['lat'].shift().fillna(track['lat'])\n",
    "\n",
    "    track['dt'] = (track['time'] - track['time'].shift()).dt.seconds\n",
    "    track['dt'] = track['dt'].fillna(0).astype('int64')\n",
    "\n",
    "    track['dist'] = haversine_distance_gpu(track['lon'], track['lat'],\n",
    "                                           track['lon_prev'], track['lat_prev'])\n",
    "\n",
    "    # filter junmp\n",
    "    mask_jump = (track['dt'] <= 60) & (track['dist'] > 200)\n",
    "    track = track[~mask_jump]\n",
    "\n",
    "    track['idle'] = (track['velocity'] <= V_THR).astype('i1')\n",
    "\n",
    "    ce = ce.sort_values('start_time')\n",
    "    ce['prev_end'] = ce['end_time'].shift()\n",
    "\n",
    "    out = []\n",
    "    for r in ce.to_pandas().itertuples(index=False):\n",
    "        if pd.isna(r.prev_end):           # 第一条充电无前区间\n",
    "            continue\n",
    "        seg = track[(track['time'] > r.prev_end) & (track['time'] <= r.start_time)]\n",
    "\n",
    "        drive_s = int(seg.loc[seg.idle == 0, 'dt'].sum())\n",
    "        idle_s  = int(seg.loc[seg.idle == 1, 'dt'].sum())\n",
    "        dist_km = seg['dist'].sum() / 1000\n",
    "        energy  = dist_km * CONS_KWH_PER_KM + idle_s * IDLE_KW\n",
    "\n",
    "        out.append({\n",
    "            'taxiid'         : r.taxiid,\n",
    "            'charge_start'   : r.start_time,\n",
    "            'prev_end'       : r.prev_end,\n",
    "            'gap_s'          : drive_s + idle_s,\n",
    "            'drive_s'        : drive_s,\n",
    "            'idle_s'         : idle_s,\n",
    "            'distance_km'    : dist_km,\n",
    "            'energy_used_kWh': energy\n",
    "        })\n",
    "    return cudf.DataFrame(out)\n"
   ],
   "outputs": [],
   "execution_count": 165
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T18:12:47.869046Z",
     "start_time": "2025-11-10T18:12:47.858527Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def post_charge_pickup(track: cudf.DataFrame,\n",
    "                       ce: cudf.DataFrame,\n",
    "                       max_pick_min: int = 10) -> cudf.DataFrame:\n",
    "    \"\"\"\n",
    "    计算每次充电结束后 max_pick_min 分钟内是否接客，\n",
    "    返回 wait_min 与直线距离 dist_m。\n",
    "    \"\"\"\n",
    "    # ---------- 1. 预处理轨迹 ----------\n",
    "    track = track.sort_values('time')\n",
    "    hired = track[track.passenger > 0]         # 载客点\n",
    "    if hired.empty or ce.empty:\n",
    "        return cudf.DataFrame()                # 无数据则直接返回空表\n",
    "\n",
    "    # ---------- 2. 转 pandas（充电事件通常几十条，CPU 迭代即可） ----------\n",
    "    hired_pdf = hired.to_pandas()              # 小数据，CPU 处理更灵活\n",
    "    ce_pdf    = ce.to_pandas()\n",
    "\n",
    "    res = []\n",
    "    for c in ce_pdf.itertuples(index=False):\n",
    "        win_end = c.end_time + pd.Timedelta(minutes=max_pick_min)\n",
    "        tmp = hired_pdf[(hired_pdf.time > c.end_time) & (hired_pdf.time <= win_end)]\n",
    "        if tmp.empty:\n",
    "            res.append({'taxiid': c.taxiid,\n",
    "                        'charge_end': c.end_time,\n",
    "                        'found': False})\n",
    "            continue\n",
    "\n",
    "        pick = tmp.iloc[0]                     # 第一条载客\n",
    "        wait_min = (pick.time - c.end_time).total_seconds() / 60\n",
    "        dist_m   = float(\n",
    "            haversine_distance_gpu(\n",
    "                cp.array([c.stay_lon]), cp.array([c.stay_lat]),\n",
    "                cp.array([pick.lon]),   cp.array([pick.lat])\n",
    "            )[0]\n",
    "        )\n",
    "\n",
    "        res.append({'taxiid'     : c.taxiid,\n",
    "                    'charge_end' : c.end_time,\n",
    "                    'pickup_time': pick.time,\n",
    "                    'wait_min'   : wait_min,\n",
    "                    'dist_m'     : dist_m,\n",
    "                    'found'      : True})\n",
    "\n",
    "    return cudf.DataFrame(res)\n"
   ],
   "outputs": [],
   "execution_count": 166
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T18:13:56.216162Z",
     "start_time": "2025-11-10T18:13:56.011096Z"
    }
   },
   "cell_type": "code",
   "source": [
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format=\"%(asctime)s | %(levelname)s | %(message)s\")\n",
    "engine = sql_engine()\n",
    "\n",
    "ce_all = cudf.read_parquet(char_par_path)\n",
    "taxis_all  = ce_all.taxiid.unique().to_arrow().to_pylist()\n",
    "logging.info(f'Total vehicles: {len(taxis_all)}')\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-10 19:13:56,213 | INFO | Total vehicles: 19496\n"
     ]
    }
   ],
   "execution_count": 172
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T18:14:01.994339Z",
     "start_time": "2025-11-10T18:14:01.988223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "taxis=taxis_all[3:4]\n",
    "taxis"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UUUB0C0Q5']"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 173
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T18:14:04.876247Z",
     "start_time": "2025-11-10T18:14:04.872846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "os.makedirs(TMP_DIR, exist_ok=True)\n",
    "os.makedirs(DS_ENERGY, exist_ok=True)\n",
    "os.makedirs(DS_PICK,   exist_ok=True)"
   ],
   "outputs": [],
   "execution_count": 174
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T18:15:40.520571Z",
     "start_time": "2025-11-10T18:14:06.692439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "total  = len(taxis)\n",
    "bar = tqdm(total=total, desc='Processing', unit='car')\n",
    "with ThreadPoolExecutor(max_workers=BATCH_DL_THREADS) as pool:\n",
    "    fut_map = {pool.submit(download_one, tid,\n",
    "                           f'{TMP_DIR}/{tid}.parquet', engine, table_name): tid\n",
    "               for tid in taxis}\n",
    "\n",
    "    for fut in as_completed(fut_map):\n",
    "        tid = fut_map[fut]\n",
    "        try:\n",
    "            path = fut.result()                       # 下载 + 转 Parquet\n",
    "            track = cudf.read_parquet(path)\n",
    "            ce_t  = ce_all[ce_all.taxiid == tid]\n",
    "\n",
    "            energy_df = energy_between_charges(track, ce_t)\n",
    "            pickup_df = post_charge_pickup(track, ce_t)\n",
    "\n",
    "            if not energy_df.empty:\n",
    "                pq.write_to_dataset(energy_df.to_arrow(), DS_ENERGY,\n",
    "                                    partition_cols=['taxiid'])\n",
    "            if not pickup_df.empty:\n",
    "                pq.write_to_dataset(pickup_df.to_arrow(), DS_PICK,\n",
    "                                    partition_cols=['taxiid'])\n",
    "\n",
    "            logging.info(f'{tid} ✓ done')\n",
    "        except Exception as e:\n",
    "            logging.error(f'{tid} failed: {e}')\n",
    "        finally:\n",
    "            bar.update(1)\n",
    "            cp._default_memory_pool.free_all_blocks()\n",
    "            # try:\n",
    "            #     os.remove(path)\n",
    "            # except Exception:\n",
    "            #     pass\n",
    "\n",
    "bar.close()\n",
    "logging.info('All done.')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/19496 [00:00<?, ?car/s]2025-11-10 19:15:40,514 | INFO | UUUB0C0Q5 ✓ done\n",
      "Processing:   0%|          | 1/19496 [01:33<508:02:56, 93.82s/car]\n",
      "2025-11-10 19:15:40,517 | INFO | All done.\n"
     ]
    }
   ],
   "execution_count": 175
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T18:16:03.233777Z",
     "start_time": "2025-11-10T18:16:03.201373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "TID   = 'UUUB0C0Q5'\n",
    "DIR_E = f'energy_gap.parquet/taxiid={TID}'\n",
    "DIR_P = f'post_charge_pickup.parquet/taxiid={TID}'\n",
    "\n",
    "\n",
    "energy_df = pq.ParquetDataset(DIR_E).read_pandas().to_pandas()\n",
    "print('consumption:',energy_df)\n",
    "\n",
    "\n",
    "pickup_df = pq.ParquetDataset(DIR_P).read_pandas().to_pandas()\n",
    "print('post:',pickup_df)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "consumption:           charge_start            prev_end   gap_s  drive_s  idle_s  \\\n",
      "0  2020-01-01 07:01:26 2020-01-01 04:23:40    8619     1183    7436   \n",
      "1  2020-01-01 09:47:26 2020-01-01 07:20:53    7322     6769     553   \n",
      "2  2020-01-03 00:25:35 2020-01-01 10:26:56  121570    38725   82845   \n",
      "3  2020-01-03 05:39:57 2020-01-03 00:40:48   16691     3445   13246   \n",
      "4  2020-01-03 12:15:27 2020-01-03 06:29:27   16544     9411    7133   \n",
      "5  2020-01-04 06:29:24 2020-01-03 12:33:04   57331    21677   35654   \n",
      "6  2020-01-05 06:36:46 2020-01-04 06:41:38   71741    31589   40152   \n",
      "7  2020-01-06 07:02:52 2020-01-05 06:48:34   77436    25738   51698   \n",
      "8  2020-01-06 22:55:18 2020-01-06 07:17:24   46836    27147   19689   \n",
      "9  2020-01-07 04:56:28 2020-01-06 23:16:18   18510     4666   13844   \n",
      "10 2020-01-07 06:44:58 2020-01-07 05:18:43    4273     1538    2735   \n",
      "\n",
      "    distance_km  energy_used_kWh  \n",
      "0     10.307915         1.855425  \n",
      "1     24.018485         4.323327  \n",
      "2    295.093654        53.116858  \n",
      "3      9.772693         1.759085  \n",
      "4     71.572503        12.883051  \n",
      "5    143.974122        25.915342  \n",
      "6    209.925378        37.786568  \n",
      "7    167.021525        30.063874  \n",
      "8    186.373648        33.547257  \n",
      "9     34.077723         6.133990  \n",
      "10    10.750931         1.935168  \n",
      "post:             charge_end  found         pickup_time  wait_min      dist_m\n",
      "0  2020-01-01 04:23:40  False                 NaT      <NA>        <NA>\n",
      "1  2020-01-01 07:20:53   True 2020-01-01 07:20:56      0.05    9.968648\n",
      "2  2020-01-01 10:26:56   True 2020-01-01 10:28:11      1.25   79.964975\n",
      "3  2020-01-03 00:40:48  False                 NaT      <NA>        <NA>\n",
      "4  2020-01-03 06:29:27   True 2020-01-03 06:29:33       0.1   71.732274\n",
      "5  2020-01-03 12:33:04   True 2020-01-03 12:33:07      0.05   13.621597\n",
      "6  2020-01-04 06:41:38  False                 NaT      <NA>        <NA>\n",
      "7  2020-01-05 06:48:34   True 2020-01-05 06:49:01      0.45   30.229117\n",
      "8  2020-01-06 07:17:24   True 2020-01-06 07:17:52  0.466667  186.264893\n",
      "9  2020-01-06 23:16:18  False                 NaT      <NA>        <NA>\n",
      "10 2020-01-07 05:18:43   True 2020-01-07 05:18:45  0.033333    4.625163\n",
      "11 2020-01-07 07:02:58   True 2020-01-07 07:03:43      0.75    5.077181\n"
     ]
    }
   ],
   "execution_count": 176
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T18:17:46.322104Z",
     "start_time": "2025-11-10T18:17:46.305532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# ⚙️ 想查看的 taxiid\n",
    "\n",
    "\n",
    "# 1️⃣ 过滤并按时间排序（GPU cuDF）\n",
    "ce_tid = charging_events[charging_events.taxiid == TID] \\\n",
    "           .sort_values('start_time')\n",
    "\n",
    "print(f'Found {len(ce_tid)} charging events for {TID}')\n",
    "\n",
    "ce_tid"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 charging events for UUUB0C0Q5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "             taxiid  stay_tag          start_time            end_time  \\\n",
       "4032375   UUUB0C0Q5      2731 2020-01-01 04:11:55 2020-01-01 04:23:40   \n",
       "10108106  UUUB0C0Q5      2737 2020-01-01 07:01:26 2020-01-01 07:20:53   \n",
       "7442462   UUUB0C0Q5      2743 2020-01-01 09:47:26 2020-01-01 10:26:56   \n",
       "14544185  UUUB0C0Q5      2910 2020-01-03 00:25:35 2020-01-03 00:40:48   \n",
       "14347474  UUUB0C0Q5      2918 2020-01-03 05:39:57 2020-01-03 06:29:27   \n",
       "10949258  UUUB0C0Q5      2951 2020-01-03 12:15:27 2020-01-03 12:33:04   \n",
       "9044234   UUUB0C0Q5      3021 2020-01-04 06:29:24 2020-01-04 06:41:38   \n",
       "6361262   UUUB0C0Q5      3152 2020-01-05 06:36:46 2020-01-05 06:48:34   \n",
       "3904164   UUUB0C0Q5      3262 2020-01-06 07:02:52 2020-01-06 07:17:24   \n",
       "9424026   UUUB0C0Q5      3342 2020-01-06 22:55:18 2020-01-06 23:16:18   \n",
       "15488264  UUUB0C0Q5      3372 2020-01-07 04:56:28 2020-01-07 05:18:43   \n",
       "14587049  UUUB0C0Q5      3387 2020-01-07 06:44:58 2020-01-07 07:02:58   \n",
       "\n",
       "            stay_lon   stay_lat  points  duration_s  distance_to_station  \\\n",
       "4032375   114.123706  22.567409      15         705            60.081880   \n",
       "10108106  114.107730  22.569469      38        1167           120.196760   \n",
       "7442462   114.061849  22.518453      72        2370            97.994113   \n",
       "14544185  114.123779  22.567329      25         913            72.014360   \n",
       "14347474  114.113932  22.534414     129        2970           138.370543   \n",
       "10949258  114.020027  22.532431      31        1057           183.203650   \n",
       "9044234   114.013931  22.545993      33         734           113.087207   \n",
       "6361262   114.104235  22.552713      38         708           145.967162   \n",
       "3904164   114.069562  22.526544      27         872           192.937863   \n",
       "9424026   114.115179  22.539300      22        1260           184.194632   \n",
       "15488264  114.104376  22.552703      63        1335           130.561280   \n",
       "14587049  114.069580  22.526529      29        1080           191.723045   \n",
       "\n",
       "          nearest_station_id  \n",
       "4032375                 2208  \n",
       "10108106                2101  \n",
       "7442462                 1923  \n",
       "14544185                2208  \n",
       "14347474                2143  \n",
       "10949258                1643  \n",
       "9044234                 1616  \n",
       "6361262                 2094  \n",
       "3904164                 2003  \n",
       "9424026                 2145  \n",
       "15488264                2094  \n",
       "14587049                2003  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>taxiid</th>\n",
       "      <th>stay_tag</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>stay_lon</th>\n",
       "      <th>stay_lat</th>\n",
       "      <th>points</th>\n",
       "      <th>duration_s</th>\n",
       "      <th>distance_to_station</th>\n",
       "      <th>nearest_station_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4032375</th>\n",
       "      <td>UUUB0C0Q5</td>\n",
       "      <td>2731</td>\n",
       "      <td>2020-01-01 04:11:55</td>\n",
       "      <td>2020-01-01 04:23:40</td>\n",
       "      <td>114.123706</td>\n",
       "      <td>22.567409</td>\n",
       "      <td>15</td>\n",
       "      <td>705</td>\n",
       "      <td>60.081880</td>\n",
       "      <td>2208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10108106</th>\n",
       "      <td>UUUB0C0Q5</td>\n",
       "      <td>2737</td>\n",
       "      <td>2020-01-01 07:01:26</td>\n",
       "      <td>2020-01-01 07:20:53</td>\n",
       "      <td>114.107730</td>\n",
       "      <td>22.569469</td>\n",
       "      <td>38</td>\n",
       "      <td>1167</td>\n",
       "      <td>120.196760</td>\n",
       "      <td>2101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7442462</th>\n",
       "      <td>UUUB0C0Q5</td>\n",
       "      <td>2743</td>\n",
       "      <td>2020-01-01 09:47:26</td>\n",
       "      <td>2020-01-01 10:26:56</td>\n",
       "      <td>114.061849</td>\n",
       "      <td>22.518453</td>\n",
       "      <td>72</td>\n",
       "      <td>2370</td>\n",
       "      <td>97.994113</td>\n",
       "      <td>1923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14544185</th>\n",
       "      <td>UUUB0C0Q5</td>\n",
       "      <td>2910</td>\n",
       "      <td>2020-01-03 00:25:35</td>\n",
       "      <td>2020-01-03 00:40:48</td>\n",
       "      <td>114.123779</td>\n",
       "      <td>22.567329</td>\n",
       "      <td>25</td>\n",
       "      <td>913</td>\n",
       "      <td>72.014360</td>\n",
       "      <td>2208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14347474</th>\n",
       "      <td>UUUB0C0Q5</td>\n",
       "      <td>2918</td>\n",
       "      <td>2020-01-03 05:39:57</td>\n",
       "      <td>2020-01-03 06:29:27</td>\n",
       "      <td>114.113932</td>\n",
       "      <td>22.534414</td>\n",
       "      <td>129</td>\n",
       "      <td>2970</td>\n",
       "      <td>138.370543</td>\n",
       "      <td>2143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10949258</th>\n",
       "      <td>UUUB0C0Q5</td>\n",
       "      <td>2951</td>\n",
       "      <td>2020-01-03 12:15:27</td>\n",
       "      <td>2020-01-03 12:33:04</td>\n",
       "      <td>114.020027</td>\n",
       "      <td>22.532431</td>\n",
       "      <td>31</td>\n",
       "      <td>1057</td>\n",
       "      <td>183.203650</td>\n",
       "      <td>1643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9044234</th>\n",
       "      <td>UUUB0C0Q5</td>\n",
       "      <td>3021</td>\n",
       "      <td>2020-01-04 06:29:24</td>\n",
       "      <td>2020-01-04 06:41:38</td>\n",
       "      <td>114.013931</td>\n",
       "      <td>22.545993</td>\n",
       "      <td>33</td>\n",
       "      <td>734</td>\n",
       "      <td>113.087207</td>\n",
       "      <td>1616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6361262</th>\n",
       "      <td>UUUB0C0Q5</td>\n",
       "      <td>3152</td>\n",
       "      <td>2020-01-05 06:36:46</td>\n",
       "      <td>2020-01-05 06:48:34</td>\n",
       "      <td>114.104235</td>\n",
       "      <td>22.552713</td>\n",
       "      <td>38</td>\n",
       "      <td>708</td>\n",
       "      <td>145.967162</td>\n",
       "      <td>2094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3904164</th>\n",
       "      <td>UUUB0C0Q5</td>\n",
       "      <td>3262</td>\n",
       "      <td>2020-01-06 07:02:52</td>\n",
       "      <td>2020-01-06 07:17:24</td>\n",
       "      <td>114.069562</td>\n",
       "      <td>22.526544</td>\n",
       "      <td>27</td>\n",
       "      <td>872</td>\n",
       "      <td>192.937863</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9424026</th>\n",
       "      <td>UUUB0C0Q5</td>\n",
       "      <td>3342</td>\n",
       "      <td>2020-01-06 22:55:18</td>\n",
       "      <td>2020-01-06 23:16:18</td>\n",
       "      <td>114.115179</td>\n",
       "      <td>22.539300</td>\n",
       "      <td>22</td>\n",
       "      <td>1260</td>\n",
       "      <td>184.194632</td>\n",
       "      <td>2145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15488264</th>\n",
       "      <td>UUUB0C0Q5</td>\n",
       "      <td>3372</td>\n",
       "      <td>2020-01-07 04:56:28</td>\n",
       "      <td>2020-01-07 05:18:43</td>\n",
       "      <td>114.104376</td>\n",
       "      <td>22.552703</td>\n",
       "      <td>63</td>\n",
       "      <td>1335</td>\n",
       "      <td>130.561280</td>\n",
       "      <td>2094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14587049</th>\n",
       "      <td>UUUB0C0Q5</td>\n",
       "      <td>3387</td>\n",
       "      <td>2020-01-07 06:44:58</td>\n",
       "      <td>2020-01-07 07:02:58</td>\n",
       "      <td>114.069580</td>\n",
       "      <td>22.526529</td>\n",
       "      <td>29</td>\n",
       "      <td>1080</td>\n",
       "      <td>191.723045</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 177
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T18:17:47.900351Z",
     "start_time": "2025-11-10T18:17:47.883056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "st_tid = final_stays[final_stays.taxiid == TID] \\\n",
    "           .sort_values('start_time')\n",
    "\n",
    "print(f'Found {len(st_tid)} stay events for {TID}')\n",
    "\n",
    "st_tid"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30 stay events for UUUB0C0Q5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "             taxiid  stay_tag          start_time            end_time  \\\n",
       "4032375   UUUB0C0Q5      2731 2020-01-01 04:11:55 2020-01-01 04:23:40   \n",
       "8730334   UUUB0C0Q5      2733 2020-01-01 04:56:43 2020-01-01 05:20:58   \n",
       "10108106  UUUB0C0Q5      2737 2020-01-01 07:01:26 2020-01-01 07:20:53   \n",
       "7442462   UUUB0C0Q5      2743 2020-01-01 09:47:26 2020-01-01 10:26:56   \n",
       "3806538   UUUB0C0Q5      2792 2020-01-02 06:20:19 2020-01-02 06:41:04   \n",
       "1471595   UUUB0C0Q5      2814 2020-01-02 10:57:21 2020-01-02 11:10:04   \n",
       "5494153   UUUB0C0Q5      2841 2020-01-02 14:02:06 2020-01-02 14:17:51   \n",
       "16769201  UUUB0C0Q5      2854 2020-01-02 16:31:51 2020-01-02 16:49:36   \n",
       "14544185  UUUB0C0Q5      2910 2020-01-03 00:25:35 2020-01-03 00:40:48   \n",
       "14347474  UUUB0C0Q5      2918 2020-01-03 05:39:57 2020-01-03 06:29:27   \n",
       "3156606   UUUB0C0Q5      2939 2020-01-03 09:00:12 2020-01-03 09:10:27   \n",
       "10949258  UUUB0C0Q5      2951 2020-01-03 12:15:27 2020-01-03 12:33:04   \n",
       "4847278   UUUB0C0Q5      2952 2020-01-03 12:49:12 2020-01-03 13:04:23   \n",
       "4206746   UUUB0C0Q5      2954 2020-01-03 13:26:42 2020-01-03 13:38:42   \n",
       "9044234   UUUB0C0Q5      3021 2020-01-04 06:29:24 2020-01-04 06:41:38   \n",
       "3029341   UUUB0C0Q5      3027 2020-01-04 07:22:39 2020-01-04 07:49:34   \n",
       "14087529  UUUB0C0Q5      3033 2020-01-04 09:26:02 2020-01-04 09:52:43   \n",
       "5527718   UUUB0C0Q5      3035 2020-01-04 09:59:12 2020-01-04 10:09:13   \n",
       "1797858   UUUB0C0Q5      3068 2020-01-04 14:41:28 2020-01-04 15:30:23   \n",
       "6361262   UUUB0C0Q5      3152 2020-01-05 06:36:46 2020-01-05 06:48:34   \n",
       "7856861   UUUB0C0Q5      3159 2020-01-05 07:51:16 2020-01-05 08:03:16   \n",
       "8909059   UUUB0C0Q5      3253 2020-01-06 01:14:47 2020-01-06 01:26:32   \n",
       "14256205  UUUB0C0Q5      3256 2020-01-06 05:37:30 2020-01-06 06:18:22   \n",
       "3904164   UUUB0C0Q5      3262 2020-01-06 07:02:52 2020-01-06 07:17:24   \n",
       "14039518  UUUB0C0Q5      3303 2020-01-06 13:56:52 2020-01-06 14:08:37   \n",
       "9424026   UUUB0C0Q5      3342 2020-01-06 22:55:18 2020-01-06 23:16:18   \n",
       "15488264  UUUB0C0Q5      3372 2020-01-07 04:56:28 2020-01-07 05:18:43   \n",
       "16664632  UUUB0C0Q5      3384 2020-01-07 05:56:58 2020-01-07 06:30:51   \n",
       "14587049  UUUB0C0Q5      3387 2020-01-07 06:44:58 2020-01-07 07:02:58   \n",
       "5864113   UUUB0C0Q5      3408 2020-01-07 11:55:02 2020-01-07 12:18:21   \n",
       "\n",
       "            stay_lon   stay_lat  points  duration_s  distance_to_station  \\\n",
       "4032375   114.123706  22.567409      15         705            60.081880   \n",
       "8730334   114.123061  22.611212      67        1455           205.938181   \n",
       "10108106  114.107730  22.569469      38        1167           120.196760   \n",
       "7442462   114.061849  22.518453      72        2370            97.994113   \n",
       "3806538   114.111741  22.597628      52        1245           265.598726   \n",
       "1471595   113.989063  22.527947      25         763           821.825154   \n",
       "5494153   113.989038  22.527635      20         945           855.331730   \n",
       "16769201  114.122949  22.611162      25        1065           216.518544   \n",
       "14544185  114.123779  22.567329      25         913            72.014360   \n",
       "14347474  114.113932  22.534414     129        2970           138.370543   \n",
       "3156606   114.050326  22.540482      22         615           226.700163   \n",
       "10949258  114.020027  22.532431      31        1057           183.203650   \n",
       "4847278   113.945160  22.506935      24         911           483.374007   \n",
       "4206746   113.947529  22.576658      26         720           553.711084   \n",
       "9044234   114.013931  22.545993      33         734           113.087207   \n",
       "3029341   113.945041  22.506915      45        1615           476.509022   \n",
       "14087529  113.813097  22.625620      45        1601          2648.031789   \n",
       "5527718   113.809977  22.627201      15         601          2914.002410   \n",
       "1797858   114.136623  22.634821      92        2935           931.049995   \n",
       "6361262   114.104235  22.552713      38         708           145.967162   \n",
       "7856861   114.015686  22.540118      20         720           255.499257   \n",
       "8909059   114.122925  22.611298      17         705           221.714199   \n",
       "14256205  114.111477  22.597570     105        2452           271.881606   \n",
       "3904164   114.069562  22.526544      27         872           192.937863   \n",
       "14039518  114.111850  22.531025      22         705           354.441200   \n",
       "9424026   114.115179  22.539300      22        1260           184.194632   \n",
       "15488264  114.104376  22.552703      63        1335           130.561280   \n",
       "16664632  114.069383  22.526522      83        2033           212.293769   \n",
       "14587049  114.069580  22.526529      29        1080           191.723045   \n",
       "5864113   113.812202  22.626178      27        1399          2717.141031   \n",
       "\n",
       "          nearest_station_id  \n",
       "4032375                 2208  \n",
       "8730334                 2225  \n",
       "10108106                2101  \n",
       "7442462                 1923  \n",
       "3806538                 2142  \n",
       "1471595                 1560  \n",
       "5494153                 1560  \n",
       "16769201                2190  \n",
       "14544185                2208  \n",
       "14347474                2143  \n",
       "3156606                 1847  \n",
       "10949258                1643  \n",
       "4847278                 1469  \n",
       "4206746                 1475  \n",
       "9044234                 1616  \n",
       "3029341                 1469  \n",
       "14087529                1071  \n",
       "5527718                 1071  \n",
       "1797858                 2308  \n",
       "6361262                 2094  \n",
       "7856861                 1619  \n",
       "8909059                 2190  \n",
       "14256205                2142  \n",
       "3904164                 2003  \n",
       "14039518                2143  \n",
       "9424026                 2145  \n",
       "15488264                2094  \n",
       "16664632                2003  \n",
       "14587049                2003  \n",
       "5864113                 1071  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>taxiid</th>\n",
       "      <th>stay_tag</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>stay_lon</th>\n",
       "      <th>stay_lat</th>\n",
       "      <th>points</th>\n",
       "      <th>duration_s</th>\n",
       "      <th>distance_to_station</th>\n",
       "      <th>nearest_station_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4032375</th>\n",
       "      <td>UUUB0C0Q5</td>\n",
       "      <td>2731</td>\n",
       "      <td>2020-01-01 04:11:55</td>\n",
       "      <td>2020-01-01 04:23:40</td>\n",
       "      <td>114.123706</td>\n",
       "      <td>22.567409</td>\n",
       "      <td>15</td>\n",
       "      <td>705</td>\n",
       "      <td>60.081880</td>\n",
       "      <td>2208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8730334</th>\n",
       "      <td>UUUB0C0Q5</td>\n",
       "      <td>2733</td>\n",
       "      <td>2020-01-01 04:56:43</td>\n",
       "      <td>2020-01-01 05:20:58</td>\n",
       "      <td>114.123061</td>\n",
       "      <td>22.611212</td>\n",
       "      <td>67</td>\n",
       "      <td>1455</td>\n",
       "      <td>205.938181</td>\n",
       "      <td>2225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10108106</th>\n",
       "      <td>UUUB0C0Q5</td>\n",
       "      <td>2737</td>\n",
       "      <td>2020-01-01 07:01:26</td>\n",
       "      <td>2020-01-01 07:20:53</td>\n",
       "      <td>114.107730</td>\n",
       "      <td>22.569469</td>\n",
       "      <td>38</td>\n",
       "      <td>1167</td>\n",
       "      <td>120.196760</td>\n",
       "      <td>2101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7442462</th>\n",
       "      <td>UUUB0C0Q5</td>\n",
       "      <td>2743</td>\n",
       "      <td>2020-01-01 09:47:26</td>\n",
       "      <td>2020-01-01 10:26:56</td>\n",
       "      <td>114.061849</td>\n",
       "      <td>22.518453</td>\n",
       "      <td>72</td>\n",
       "      <td>2370</td>\n",
       "      <td>97.994113</td>\n",
       "      <td>1923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3806538</th>\n",
       "      <td>UUUB0C0Q5</td>\n",
       "      <td>2792</td>\n",
       "      <td>2020-01-02 06:20:19</td>\n",
       "      <td>2020-01-02 06:41:04</td>\n",
       "      <td>114.111741</td>\n",
       "      <td>22.597628</td>\n",
       "      <td>52</td>\n",
       "      <td>1245</td>\n",
       "      <td>265.598726</td>\n",
       "      <td>2142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1471595</th>\n",
       "      <td>UUUB0C0Q5</td>\n",
       "      <td>2814</td>\n",
       "      <td>2020-01-02 10:57:21</td>\n",
       "      <td>2020-01-02 11:10:04</td>\n",
       "      <td>113.989063</td>\n",
       "      <td>22.527947</td>\n",
       "      <td>25</td>\n",
       "      <td>763</td>\n",
       "      <td>821.825154</td>\n",
       "      <td>1560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5494153</th>\n",
       "      <td>UUUB0C0Q5</td>\n",
       "      <td>2841</td>\n",
       "      <td>2020-01-02 14:02:06</td>\n",
       "      <td>2020-01-02 14:17:51</td>\n",
       "      <td>113.989038</td>\n",
       "      <td>22.527635</td>\n",
       "      <td>20</td>\n",
       "      <td>945</td>\n",
       "      <td>855.331730</td>\n",
       "      <td>1560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16769201</th>\n",
       "      <td>UUUB0C0Q5</td>\n",
       "      <td>2854</td>\n",
       "      <td>2020-01-02 16:31:51</td>\n",
       "      <td>2020-01-02 16:49:36</td>\n",
       "      <td>114.122949</td>\n",
       "      <td>22.611162</td>\n",
       "      <td>25</td>\n",
       "      <td>1065</td>\n",
       "      <td>216.518544</td>\n",
       "      <td>2190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14544185</th>\n",
       "      <td>UUUB0C0Q5</td>\n",
       "      <td>2910</td>\n",
       "      <td>2020-01-03 00:25:35</td>\n",
       "      <td>2020-01-03 00:40:48</td>\n",
       "      <td>114.123779</td>\n",
       "      <td>22.567329</td>\n",
       "      <td>25</td>\n",
       "      <td>913</td>\n",
       "      <td>72.014360</td>\n",
       "      <td>2208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14347474</th>\n",
       "      <td>UUUB0C0Q5</td>\n",
       "      <td>2918</td>\n",
       "      <td>2020-01-03 05:39:57</td>\n",
       "      <td>2020-01-03 06:29:27</td>\n",
       "      <td>114.113932</td>\n",
       "      <td>22.534414</td>\n",
       "      <td>129</td>\n",
       "      <td>2970</td>\n",
       "      <td>138.370543</td>\n",
       "      <td>2143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3156606</th>\n",
       "      <td>UUUB0C0Q5</td>\n",
       "      <td>2939</td>\n",
       "      <td>2020-01-03 09:00:12</td>\n",
       "      <td>2020-01-03 09:10:27</td>\n",
       "      <td>114.050326</td>\n",
       "      <td>22.540482</td>\n",
       "      <td>22</td>\n",
       "      <td>615</td>\n",
       "      <td>226.700163</td>\n",
       "      <td>1847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10949258</th>\n",
       "      <td>UUUB0C0Q5</td>\n",
       "      <td>2951</td>\n",
       "      <td>2020-01-03 12:15:27</td>\n",
       "      <td>2020-01-03 12:33:04</td>\n",
       "      <td>114.020027</td>\n",
       "      <td>22.532431</td>\n",
       "      <td>31</td>\n",
       "      <td>1057</td>\n",
       "      <td>183.203650</td>\n",
       "      <td>1643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4847278</th>\n",
       "      <td>UUUB0C0Q5</td>\n",
       "      <td>2952</td>\n",
       "      <td>2020-01-03 12:49:12</td>\n",
       "      <td>2020-01-03 13:04:23</td>\n",
       "      <td>113.945160</td>\n",
       "      <td>22.506935</td>\n",
       "      <td>24</td>\n",
       "      <td>911</td>\n",
       "      <td>483.374007</td>\n",
       "      <td>1469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4206746</th>\n",
       "      <td>UUUB0C0Q5</td>\n",
       "      <td>2954</td>\n",
       "      <td>2020-01-03 13:26:42</td>\n",
       "      <td>2020-01-03 13:38:42</td>\n",
       "      <td>113.947529</td>\n",
       "      <td>22.576658</td>\n",
       "      <td>26</td>\n",
       "      <td>720</td>\n",
       "      <td>553.711084</td>\n",
       "      <td>1475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9044234</th>\n",
       "      <td>UUUB0C0Q5</td>\n",
       "      <td>3021</td>\n",
       "      <td>2020-01-04 06:29:24</td>\n",
       "      <td>2020-01-04 06:41:38</td>\n",
       "      <td>114.013931</td>\n",
       "      <td>22.545993</td>\n",
       "      <td>33</td>\n",
       "      <td>734</td>\n",
       "      <td>113.087207</td>\n",
       "      <td>1616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3029341</th>\n",
       "      <td>UUUB0C0Q5</td>\n",
       "      <td>3027</td>\n",
       "      <td>2020-01-04 07:22:39</td>\n",
       "      <td>2020-01-04 07:49:34</td>\n",
       "      <td>113.945041</td>\n",
       "      <td>22.506915</td>\n",
       "      <td>45</td>\n",
       "      <td>1615</td>\n",
       "      <td>476.509022</td>\n",
       "      <td>1469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14087529</th>\n",
       "      <td>UUUB0C0Q5</td>\n",
       "      <td>3033</td>\n",
       "      <td>2020-01-04 09:26:02</td>\n",
       "      <td>2020-01-04 09:52:43</td>\n",
       "      <td>113.813097</td>\n",
       "      <td>22.625620</td>\n",
       "      <td>45</td>\n",
       "      <td>1601</td>\n",
       "      <td>2648.031789</td>\n",
       "      <td>1071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5527718</th>\n",
       "      <td>UUUB0C0Q5</td>\n",
       "      <td>3035</td>\n",
       "      <td>2020-01-04 09:59:12</td>\n",
       "      <td>2020-01-04 10:09:13</td>\n",
       "      <td>113.809977</td>\n",
       "      <td>22.627201</td>\n",
       "      <td>15</td>\n",
       "      <td>601</td>\n",
       "      <td>2914.002410</td>\n",
       "      <td>1071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797858</th>\n",
       "      <td>UUUB0C0Q5</td>\n",
       "      <td>3068</td>\n",
       "      <td>2020-01-04 14:41:28</td>\n",
       "      <td>2020-01-04 15:30:23</td>\n",
       "      <td>114.136623</td>\n",
       "      <td>22.634821</td>\n",
       "      <td>92</td>\n",
       "      <td>2935</td>\n",
       "      <td>931.049995</td>\n",
       "      <td>2308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6361262</th>\n",
       "      <td>UUUB0C0Q5</td>\n",
       "      <td>3152</td>\n",
       "      <td>2020-01-05 06:36:46</td>\n",
       "      <td>2020-01-05 06:48:34</td>\n",
       "      <td>114.104235</td>\n",
       "      <td>22.552713</td>\n",
       "      <td>38</td>\n",
       "      <td>708</td>\n",
       "      <td>145.967162</td>\n",
       "      <td>2094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7856861</th>\n",
       "      <td>UUUB0C0Q5</td>\n",
       "      <td>3159</td>\n",
       "      <td>2020-01-05 07:51:16</td>\n",
       "      <td>2020-01-05 08:03:16</td>\n",
       "      <td>114.015686</td>\n",
       "      <td>22.540118</td>\n",
       "      <td>20</td>\n",
       "      <td>720</td>\n",
       "      <td>255.499257</td>\n",
       "      <td>1619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8909059</th>\n",
       "      <td>UUUB0C0Q5</td>\n",
       "      <td>3253</td>\n",
       "      <td>2020-01-06 01:14:47</td>\n",
       "      <td>2020-01-06 01:26:32</td>\n",
       "      <td>114.122925</td>\n",
       "      <td>22.611298</td>\n",
       "      <td>17</td>\n",
       "      <td>705</td>\n",
       "      <td>221.714199</td>\n",
       "      <td>2190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14256205</th>\n",
       "      <td>UUUB0C0Q5</td>\n",
       "      <td>3256</td>\n",
       "      <td>2020-01-06 05:37:30</td>\n",
       "      <td>2020-01-06 06:18:22</td>\n",
       "      <td>114.111477</td>\n",
       "      <td>22.597570</td>\n",
       "      <td>105</td>\n",
       "      <td>2452</td>\n",
       "      <td>271.881606</td>\n",
       "      <td>2142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3904164</th>\n",
       "      <td>UUUB0C0Q5</td>\n",
       "      <td>3262</td>\n",
       "      <td>2020-01-06 07:02:52</td>\n",
       "      <td>2020-01-06 07:17:24</td>\n",
       "      <td>114.069562</td>\n",
       "      <td>22.526544</td>\n",
       "      <td>27</td>\n",
       "      <td>872</td>\n",
       "      <td>192.937863</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14039518</th>\n",
       "      <td>UUUB0C0Q5</td>\n",
       "      <td>3303</td>\n",
       "      <td>2020-01-06 13:56:52</td>\n",
       "      <td>2020-01-06 14:08:37</td>\n",
       "      <td>114.111850</td>\n",
       "      <td>22.531025</td>\n",
       "      <td>22</td>\n",
       "      <td>705</td>\n",
       "      <td>354.441200</td>\n",
       "      <td>2143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9424026</th>\n",
       "      <td>UUUB0C0Q5</td>\n",
       "      <td>3342</td>\n",
       "      <td>2020-01-06 22:55:18</td>\n",
       "      <td>2020-01-06 23:16:18</td>\n",
       "      <td>114.115179</td>\n",
       "      <td>22.539300</td>\n",
       "      <td>22</td>\n",
       "      <td>1260</td>\n",
       "      <td>184.194632</td>\n",
       "      <td>2145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15488264</th>\n",
       "      <td>UUUB0C0Q5</td>\n",
       "      <td>3372</td>\n",
       "      <td>2020-01-07 04:56:28</td>\n",
       "      <td>2020-01-07 05:18:43</td>\n",
       "      <td>114.104376</td>\n",
       "      <td>22.552703</td>\n",
       "      <td>63</td>\n",
       "      <td>1335</td>\n",
       "      <td>130.561280</td>\n",
       "      <td>2094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16664632</th>\n",
       "      <td>UUUB0C0Q5</td>\n",
       "      <td>3384</td>\n",
       "      <td>2020-01-07 05:56:58</td>\n",
       "      <td>2020-01-07 06:30:51</td>\n",
       "      <td>114.069383</td>\n",
       "      <td>22.526522</td>\n",
       "      <td>83</td>\n",
       "      <td>2033</td>\n",
       "      <td>212.293769</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14587049</th>\n",
       "      <td>UUUB0C0Q5</td>\n",
       "      <td>3387</td>\n",
       "      <td>2020-01-07 06:44:58</td>\n",
       "      <td>2020-01-07 07:02:58</td>\n",
       "      <td>114.069580</td>\n",
       "      <td>22.526529</td>\n",
       "      <td>29</td>\n",
       "      <td>1080</td>\n",
       "      <td>191.723045</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5864113</th>\n",
       "      <td>UUUB0C0Q5</td>\n",
       "      <td>3408</td>\n",
       "      <td>2020-01-07 11:55:02</td>\n",
       "      <td>2020-01-07 12:18:21</td>\n",
       "      <td>113.812202</td>\n",
       "      <td>22.626178</td>\n",
       "      <td>27</td>\n",
       "      <td>1399</td>\n",
       "      <td>2717.141031</td>\n",
       "      <td>1071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 178
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.histplot(energy_df['distance_km'], bins=30, kde=True)\n",
    "plt.xlabel('Distance between charges (km)')\n",
    "plt.title(f'{TID}: Distance distribution')\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.scatterplot(data=pickup_df[pickup_df['found']],\n",
    "                x='charge_end', y='wait_min')\n",
    "plt.ylabel('Wait minutes after charging')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(f'{TID}: Post-charge pickup waiting time')\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "\n",
    "m = folium.Map(location=[energy_df['stay_lat'].mean(),\n",
    "                         energy_df['stay_lon'].mean()],\n",
    "               zoom_start=12)\n",
    "cluster = MarkerCluster().add_to(m)\n",
    "for _, r in pickup_df[pickup_df['found']].iterrows():\n",
    "    folium.CircleMarker(\n",
    "        [r['stay_lat'], r['stay_lon']],\n",
    "        radius=5,color='red',fill=True,fill_opacity=0.6,\n",
    "        popup=f\"Wait {r['wait_min']:.1f} min, {r['dist_m']:.0f} m\"\n",
    "    ).add_to(cluster)\n",
    "m"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T16:54:28.818556Z",
     "start_time": "2025-11-10T16:54:27.227118Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ⚙️ 参数：想看的 taxiid\n",
    "TID = 'UUUB0C0M7'              # 改成任意车辆编号\n",
    "PARQUET_PATH = f'{TMP_DIR}/{TID}.parquet'   # 若已删除，请重新 download_one 后再运行\n",
    "\n",
    "# 1️⃣ 读取整车轨迹\n",
    "if not os.path.exists(PARQUET_PATH):\n",
    "    raise FileNotFoundError(\"先调用 download_one 或确保 Parquet 文件存在\")\n",
    "\n",
    "track = cudf.read_parquet(PARQUET_PATH) \\\n",
    "            .sort_values('time') \\\n",
    "            .to_pandas()                 # 转 pandas 便于 folium\n",
    "\n",
    "print(f'{len(track):,} points loaded for {TID}')\n",
    "\n",
    "# 2️⃣ 取中心点\n",
    "center_lat = track['lat'].mean()\n",
    "center_lon = track['lon'].mean()\n",
    "\n",
    "# 3️⃣ Folium 地图\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "\n",
    "m = folium.Map(location=[center_lat, center_lon],\n",
    "               zoom_start=11,\n",
    "               tiles='OpenStreetMap')\n",
    "\n",
    "# 3-a 轨迹折线（可选：分颜色段）\n",
    "coords = track[['lat','lon']].values.tolist()\n",
    "folium.PolyLine(coords, color='blue', weight=2, opacity=0.6,\n",
    "                tooltip=f'{TID} trajectory').add_to(m)\n",
    "\n",
    "# 3-b 标记起点终点\n",
    "folium.Marker(coords[0], icon=folium.Icon(color='green'),\n",
    "              tooltip='Start').add_to(m)\n",
    "folium.Marker(coords[-1], icon=folium.Icon(color='red'),\n",
    "              tooltip='End').add_to(m)\n",
    "\n",
    "m"
   ],
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'dist'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[0;32m~/miniconda3/envs/rapids-env/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3804\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 3805\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3806\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[0;32mindex.pyx:167\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mindex.pyx:196\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'dist'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[133], line 12\u001B[0m\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m先调用 download_one 或确保 Parquet 文件存在\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      9\u001B[0m track \u001B[38;5;241m=\u001B[39m cudf\u001B[38;5;241m.\u001B[39mread_parquet(PARQUET_PATH) \\\n\u001B[1;32m     10\u001B[0m             \u001B[38;5;241m.\u001B[39msort_values(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtime\u001B[39m\u001B[38;5;124m'\u001B[39m) \\\n\u001B[1;32m     11\u001B[0m             \u001B[38;5;241m.\u001B[39mto_pandas()                 \u001B[38;5;66;03m# 转 pandas 便于 folium\u001B[39;00m\n\u001B[0;32m---> 12\u001B[0m d \u001B[38;5;241m=\u001B[39m \u001B[43mtrack\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdist\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39mto_pandas()          \u001B[38;5;66;03m# 单位: 米\u001B[39;00m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28mprint\u001B[39m(d\u001B[38;5;241m.\u001B[39mdescribe(percentiles\u001B[38;5;241m=\u001B[39m[\u001B[38;5;241m.9\u001B[39m,\u001B[38;5;241m.99\u001B[39m]))\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(track)\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m,\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m points loaded for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mTID\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/envs/rapids-env/lib/python3.10/site-packages/pandas/core/frame.py:4102\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   4100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   4101\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[0;32m-> 4102\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   4103\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[1;32m   4104\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
      "File \u001B[0;32m~/miniconda3/envs/rapids-env/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3807\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(casted_key, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[1;32m   3808\u001B[0m         \u001B[38;5;28misinstance\u001B[39m(casted_key, abc\u001B[38;5;241m.\u001B[39mIterable)\n\u001B[1;32m   3809\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28many\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m casted_key)\n\u001B[1;32m   3810\u001B[0m     ):\n\u001B[1;32m   3811\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m InvalidIndexError(key)\n\u001B[0;32m-> 3812\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01merr\u001B[39;00m\n\u001B[1;32m   3813\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m   3814\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[1;32m   3815\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[1;32m   3816\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[1;32m   3817\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[0;31mKeyError\u001B[0m: 'dist'"
     ]
    }
   ],
   "execution_count": 133
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "CAP_KWH          = 57          # battery capacity (kWh)\n",
    "AVG_SPEED_KMH    = 25          # avg speed (km/h)\n",
    "CONS_KWH_PER_KM  = 0.18        # consumption rate (kWh/km)\n",
    "CHG_EFFICIENCY   = 1       # charging efficiency\n",
    "\n",
    "# ---------- 1. 轻量化转 pandas ----------\n",
    "ce = charging_events[            # GPU → CPU，只要必要列\n",
    "    ['taxiid', 'start_time', 'end_time',\n",
    "     'nearest_station_id', 'duration_s']].copy().to_pandas()\n",
    "\n",
    "stays = final_stays[             # GPU → CPU，只要必要列\n",
    "    ['taxiid', 'start_time', 'end_time', 'duration_s']].copy().to_pandas()\n",
    "\n",
    "stations = stations_df[['station_id', 'avg_power']].copy() # 站点平均功率\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ---------- 2. 上一次充电信息 ----------\n",
    "ce = ce.sort_values(['taxiid', 'start_time'])\n",
    "ce['prev_end'] = ce.groupby('taxiid')['end_time'].shift()\n",
    "ce['gap_s']    = (ce['start_time'] - ce['prev_end']).dt.total_seconds()\n",
    "\n",
    "# 仅对 gap_s 非空（= 不是首充电）的行建立区间\n",
    "intervals = (\n",
    "    ce[ce['gap_s'].notna()][['taxiid', 'prev_end', 'start_time']]\n",
    "        .copy()\n",
    "        .rename(columns={\n",
    "            'prev_end'  : 'int_start',   # 区间起点\n",
    "            'start_time': 'int_end'      # 区间终点\n",
    "        })\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ---------- 2. 上一次充电信息 ----------\n",
    "# ce = ce.sort_values(['taxiid', 'start_time'])\n",
    "# ce['prev_end'] = ce.groupby('taxiid')['end_time'].shift()\n",
    "# ce['gap_s']    = (ce['start_time'] - ce['prev_end']).dt.total_seconds()\n",
    "#\n",
    "# # ---------- 3. 非充电停留总时长 idle_s ----------\n",
    "# #   3.1 仅保留有 gap 的区间行\n",
    "# intervals = ce[ce['gap_s'].notna()][['taxiid', 'prev_end', 'start_time']].copy()\n",
    "# intervals.rename(columns={'prev_end': 'int_start',\n",
    "#                           'start_time': 'int_end'}, inplace=True)\n",
    "#\n",
    "# #   3.2 merge stays ⇆ intervals → 布尔过滤\n",
    "# tmp = (stays.merge(intervals, on='taxiid', how='inner')\n",
    "#              .query('start_time >= int_start and end_time <= int_end'))\n",
    "#\n",
    "# idle_df = (tmp.groupby(['taxiid', 'int_start'], sort=False)\n",
    "#                ['duration_s'].sum().reset_index())\n",
    "#\n",
    "# idle_df.rename(columns={'duration_s': 'idle_s'}, inplace=True)\n",
    "# ce = ce.merge(idle_df,\n",
    "#               left_on=['taxiid', 'prev_end'],\n",
    "#               right_on=['taxiid', 'int_start'],\n",
    "#               how='left') \\\n",
    "#        .drop(columns='int_start')\n",
    "#\n",
    "# ce['idle_s'] = ce['idle_s'].fillna(0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "stays_sorted = (\n",
    "    stays.dropna(subset=['taxiid', 'start_time', 'end_time'])\n",
    "         .sort_values(['taxiid', 'start_time'], kind='mergesort')\n",
    ")\n",
    "intervals_sorted = (\n",
    "    intervals.dropna(subset=['taxiid', 'int_start', 'int_end'])\n",
    "             .sort_values(['taxiid', 'int_start'], kind='mergesort')\n",
    ")\n",
    "intervals_sorted.head(10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(intervals_sorted[intervals_sorted['taxiid']='UUUBDZ9542'])",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test3=intervals_sorted['int_start'].isna().any()\n",
    "test3"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def has_break(s):\n",
    "    return (~s.is_monotonic_increasing)\n",
    "\n",
    "bad_taxis = intervals_sorted.groupby('taxiid')['int_start'].apply(has_break)\n",
    "print(bad_taxis[bad_taxis].index.tolist()[:10])   # 列出前几辆有问题的车"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# 0️⃣  已有的 ce / stays / intervals\n",
    "# ce          : 充电事件（含 prev_end, gap_s 已算好）\n",
    "# intervals   : ['taxiid','int_start','int_end']  ← 由 ce 派生\n",
    "# stays       : ['taxiid','start_time','end_time','duration_s']\n",
    "\n",
    "\n",
    "# 1️⃣  确保按 taxiid + 时间排序（merge_asof 要求升序）\n",
    "\n",
    "\n",
    "intervals_sorted = (\n",
    "    intervals.dropna(subset=['taxiid', 'int_start', 'int_end'])\n",
    "             .sort_values(['taxiid', 'int_start'], kind='mergesort')\n",
    ")\n",
    "# 2️⃣  merge_asof  : 对每个停留  ↔ 最近的不晚于它的 interval 起点\n",
    "merged = pd.merge_asof(\n",
    "    stays_sorted,                # left\n",
    "    intervals_sorted,            # right\n",
    "    left_on='start_time',\n",
    "    right_on='int_start',\n",
    "    by='taxiid',\n",
    "    direction='backward',        # 取“<=” 匹配\n",
    "    allow_exact_matches=True\n",
    ")\n",
    "\n",
    "# 3️⃣  过滤：停留必须完全落在 interval 内部\n",
    "mask = merged['end_time'] <= merged['int_end']\n",
    "valid_idle = merged.loc[mask, ['taxiid', 'int_start', 'duration_s']]\n",
    "\n",
    "# 4️⃣  汇总：每 (taxiid, int_start) 累加 idle_s\n",
    "idle_df = (valid_idle\n",
    "           .groupby(['taxiid', 'int_start'], sort=False)\n",
    "           ['duration_s'].sum()\n",
    "           .reset_index()\n",
    "           .rename(columns={'duration_s': 'idle_s'}))\n",
    "\n",
    "# 5️⃣  回并到 ce   (右键 = prev_end ≡ int_start)\n",
    "ce = (ce.merge(idle_df,\n",
    "               left_on=['taxiid', 'prev_end'],\n",
    "               right_on=['taxiid', 'int_start'],\n",
    "               how='left')\n",
    "        .drop(columns='int_start'))\n",
    "\n",
    "ce['idle_s'] = ce['idle_s'].fillna(0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ---------- 4. 行驶时间 / 距离 / 耗电 ----------\n",
    "ce['drive_s']  = (ce['gap_s'] - ce['idle_s']).clip(lower=0)\n",
    "ce['distance_km']     = ce['drive_s']/3600 * AVG_SPEED_KMH\n",
    "ce['energy_used_kWh'] = ce['distance_km'] * CONS_KWH_PER_KM\n",
    "\n",
    "# ---------- 5. 充入电量 ----------\n",
    "ce = ce.merge(stations, left_on='nearest_station_id',\n",
    "                        right_on='station_id', how='left')\n",
    "ce.drop(columns='station_id', inplace=True)\n",
    "\n",
    "ce['energy_in_kWh'] = ce['charge_s']/3600 * ce['avg_power'] * CHG_EFFICIENCY\n",
    "ce.drop(columns='avg_power', inplace=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ---------- 6. 逐车递推 SOC ----------\n",
    "def soc_trace(group):\n",
    "    soc_b, soc_a = [], []\n",
    "    soc_prev = CAP_KWH                                # 初始满电\n",
    "    for _, row in group.iterrows():\n",
    "        soc_before = max(0, soc_prev - row.energy_used_kWh)\n",
    "        soc_after  = min(CAP_KWH, soc_before + row.energy_in_kWh)\n",
    "        soc_b.append(soc_before)\n",
    "        soc_a.append(soc_after)\n",
    "        soc_prev = soc_after\n",
    "    group['SOC_before_kWh'] = soc_b\n",
    "    group['SOC_after_kWh']  = soc_a\n",
    "    return group\n",
    "\n",
    "ce = (ce.groupby('taxiid', sort=False, group_keys=False)\n",
    "         .apply(soc_trace))\n",
    "\n",
    "# 可选：百分比形式\n",
    "ce['SOC_before_pct'] = ce['SOC_before_kWh'] / CAP_KWH * 100\n",
    "ce['SOC_after_pct']  = ce['SOC_after_kWh']  / CAP_KWH * 100\n",
    "\n",
    "# ---------- 7. 保存 / 返回 ----------\n",
    "cols_order = [\n",
    "    'taxiid', 'start_time', 'end_time',\n",
    "    'SOC_before_kWh', 'SOC_after_kWh',\n",
    "    'energy_used_kWh', 'energy_in_kWh',\n",
    "    'distance_km', 'drive_s', 'idle_s', 'gap_s'\n",
    "]\n",
    "battery_trace_df = ce[cols_order].copy()\n",
    "battery_trace_df.to_parquet('battery_trace.parquet', index=False)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Leaving station"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def find_passenger_pickup_batch(charging_events, db_url, table_name, batch_size=100):\n",
    "    \"\"\"\n",
    "    分批查询：每次查询100辆车 - 平衡速度和内存\n",
    "    \"\"\"\n",
    "    charge_df = charging_events[['taxiid', 'end_time']].to_pandas()\n",
    "    unique_taxis = charge_df['taxiid'].unique()\n",
    "\n",
    "    # 分批处理\n",
    "    n_batches = int(np.ceil(len(unique_taxis) / batch_size))\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    conn = psycopg2.connect(db_url)\n",
    "\n",
    "    for i in range(n_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, len(unique_taxis))\n",
    "        batch_taxis = unique_taxis[start_idx:end_idx]\n",
    "\n",
    "        # 这批车辆的充电事件\n",
    "        batch_charges = charge_df[charge_df['taxiid'].isin(batch_taxis)]\n",
    "\n",
    "        if len(batch_charges) == 0:\n",
    "            continue\n",
    "\n",
    "        min_time = batch_charges['end_time'].min()\n",
    "        max_time = batch_charges['end_time'].max() + pd.Timedelta(minutes=10)\n",
    "\n",
    "        # 批量查询这批车辆\n",
    "        query = f\"\"\"\n",
    "        SELECT taxiid, time\n",
    "        FROM {table_name}\n",
    "        WHERE passenger > 0\n",
    "          AND taxiid IN ({','.join([f\"'{tid}'\" for tid in batch_taxis])})\n",
    "          AND time > '{min_time}'\n",
    "          AND time <= '{max_time}'\n",
    "        ORDER BY taxiid, time\n",
    "        \"\"\"\n",
    "\n",
    "        pickup_data = pd.read_sql(query, conn)\n",
    "\n",
    "        # 内存中匹配\n",
    "        for _, charge_row in batch_charges.iterrows():\n",
    "            taxiid = charge_row['taxiid']\n",
    "            charge_end = charge_row['end_time']\n",
    "            look_until = charge_end + pd.Timedelta(minutes=10)\n",
    "\n",
    "            taxi_pickups = pickup_data[\n",
    "                (pickup_data['taxiid'] == taxiid) &\n",
    "                (pickup_data['time'] > charge_end) &\n",
    "                (pickup_data['time'] <= look_until)\n",
    "            ]\n",
    "\n",
    "            if len(taxi_pickups) > 0:\n",
    "                first_pickup = taxi_pickups['time'].min()\n",
    "                minutes_after = (first_pickup - charge_end).total_seconds() / 60\n",
    "                all_results.append({\n",
    "                    'taxiid': taxiid,\n",
    "                    'charge_end_time': charge_end,\n",
    "                    'pickup_time': first_pickup,\n",
    "                    'minutes_after_charging': minutes_after,\n",
    "                    'found': True\n",
    "                })\n",
    "            else:\n",
    "                all_results.append({\n",
    "                    'taxiid': taxiid,\n",
    "                    'charge_end_time': charge_end,\n",
    "                    'pickup_time': None,\n",
    "                    'minutes_after_charging': None,\n",
    "                    'found': False\n",
    "                })\n",
    "\n",
    "        # 清理内存\n",
    "        del pickup_data\n",
    "        print(f\"Processed batch {i+1}/{n_batches}\")\n",
    "\n",
    "    conn.close()\n",
    "    return pd.DataFrame(all_results)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "test_events=charging_events.head(20)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "result_df = find_passenger_pickup_batch(test_events, DB_URL, table_name, batch_size=20)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "result_df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def find_passenger_pickup_sql_join(charging_events, db_url, table_name):\n",
    "    \"\"\"\n",
    "    使用SQL JOIN一次性匹配 - 让数据库做优化\n",
    "    \"\"\"\n",
    "    import psycopg2\n",
    "    import pandas as pd\n",
    "    from io import StringIO\n",
    "\n",
    "    # 准备充电事件数据\n",
    "    charge_df = charging_events[['taxiid', 'end_time']].to_pandas()\n",
    "    charge_df['look_end_time'] = charge_df['end_time'] + pd.Timedelta(minutes=10)\n",
    "    # 重命名列以匹配临时表\n",
    "    charge_df = charge_df.rename(columns={'end_time': 'charge_end_time'})\n",
    "\n",
    "    conn = psycopg2.connect(db_url)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # 创建临时表（一次性写入）\n",
    "    cur.execute(\"\"\"\n",
    "        CREATE TEMP TABLE charging_ends (\n",
    "            taxiid VARCHAR,\n",
    "            charge_end_time TIMESTAMP,\n",
    "            look_end_time TIMESTAMP\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "    # 批量插入（快速）\n",
    "    buffer = StringIO()\n",
    "    # 只输出需要的列，顺序要匹配\n",
    "    charge_df[['taxiid', 'charge_end_time', 'look_end_time']].to_csv(\n",
    "        buffer, index=False, header=False, sep='\\t'\n",
    "    )\n",
    "    buffer.seek(0)\n",
    "    cur.copy_from(\n",
    "        buffer,\n",
    "        'charging_ends',\n",
    "        columns=['taxiid', 'charge_end_time', 'look_end_time']\n",
    "    )\n",
    "\n",
    "    # 一次性JOIN查询（数据库优化）\n",
    "    query = \"\"\"\n",
    "    SELECT\n",
    "        ce.taxiid,\n",
    "        ce.charge_end_time,\n",
    "        MIN(gps.time) as pickup_time,\n",
    "        EXTRACT(EPOCH FROM (MIN(gps.time) - ce.charge_end_time)) / 60 as minutes_after\n",
    "    FROM charging_ends ce\n",
    "    LEFT JOIN {table_name} gps\n",
    "        ON gps.taxiid = ce.taxiid\n",
    "        AND gps.passenger = 1\n",
    "        AND gps.time > ce.charge_end_time\n",
    "        AND gps.time <= ce.look_end_time\n",
    "    GROUP BY ce.taxiid, ce.charge_end_time\n",
    "    ORDER BY ce.taxiid, ce.charge_end_time\n",
    "    \"\"\".format(table_name=table_name)\n",
    "\n",
    "    results = pd.read_sql(query, conn)\n",
    "\n",
    "    # 清理临时表\n",
    "    cur.execute(\"DROP TABLE charging_ends\")\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "    # 处理结果：如果没有匹配到，minutes_after会是NULL\n",
    "    results['found'] = results['pickup_time'].notna()\n",
    "    results['minutes_after_charging'] = results['minutes_after']\n",
    "\n",
    "    return results"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "test_events2=charging_events.head(10)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "result_df2 = find_passenger_pickup_sql_join(test_events2, DB_URL, table_name)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Visualization"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- 分析: 平均每日充电次数 ---\n",
    "\n",
    "print(\"--- 正在计算平均每日充电次数 ---\")\n",
    "\n",
    "if 'char_que_df' in locals() and not char_que_df.empty:\n",
    "\n",
    "    # --- 1. 提取充电日期 ---\n",
    "    # .dt.date 用于从 datetime 对象中仅提取日期部分\n",
    "    char_que_df['charge_date'] = char_que_df['arrive_time'].dt.date\n",
    "\n",
    "    # --- 2. 计算每辆车每天的成功充电次数 ---\n",
    "    # 我们只统计没有放弃 ('giveup' == False) 的充电事件\n",
    "    # daily_charges_per_taxi = char_que_df[char_que_df['giveup'] == False].groupby(['taxiid', 'charge_date']).size().reset_index\n",
    "    daily_charges_per_taxi = char_que_df.groupby(['taxiid', 'charge_date']).size().reset_index(name='daily_charge_count')\n",
    "    print(\"\\n--- 每辆车每日充电次数 (示例) ---\")\n",
    "    print(daily_charges_per_taxi.head())\n",
    "\n",
    "    # --- 3. 计算每辆车的“平均每日充电次数” ---\n",
    "    # 对上一步的结果再次 groupby，这次只按 taxiid，然后求均值\n",
    "    avg_daily_charges_per_taxi = daily_charges_per_taxi.groupby('taxiid')['daily_charge_count'].mean().reset_index(name='avg_daily_charges')\n",
    "    print(\"\\n--- 每辆车的平均每日充电次数 (示例) ---\")\n",
    "    print(avg_daily_charges_per_taxi.head())\n",
    "\n",
    "    # --- 4. 计算所有车的总平均每日充电次数 ---\n",
    "    # 直接对 'avg_daily_charges' 这一列求均值\n",
    "    overall_avg_daily_charges = avg_daily_charges_per_taxi['avg_daily_charges'].mean()\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(f\"✅ 所有车辆的平均每日充电次数为: {overall_avg_daily_charges:.2f} 次/天/车\")\n",
    "    print(\"=\"*40)\n",
    "\n",
    "else:\n",
    "    print(\"错误: 'char_que_df' 未找到或为空。请先运行队列模拟部分的单元格。\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- 分析: 停留与等待时间的分布统计 ---\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(\"--- 正在计算停留与等待时间的描述性统计 ---\")\n",
    "\n",
    "if 'char_que_df' in locals() and not char_que_df.empty:\n",
    "    # --- 1. 准备数据 ---\n",
    "    # 如果 'stay_duration_s' 不存在，则从时间戳计算\n",
    "    if 'stay_duration_s' not in char_que_df.columns:\n",
    "        char_que_df['stay_duration_s'] = (char_que_df['leave_time'] - char_que_df['arrive_time']).dt.total_seconds()\n",
    "\n",
    "    # 为了更好地可视化和理解，我们将秒转换为分钟\n",
    "    char_que_df['wait_minutes'] = char_que_df['wait_dur'] / 60\n",
    "    char_que_df['stay_duration_minutes'] = char_que_df['stay_duration_s'] / 60\n",
    "\n",
    "    # --- 2. 输出描述性统计 ---\n",
    "    print(\"\\n--- 排队时间 (分钟) 的描述性统计 ---\")\n",
    "    # 使用 .describe() 查看分布，特别是 50% (中位数), 75%, max 等\n",
    "    # percentiles 参数可以自定义需要查看的百分位数\n",
    "    print(char_que_df['wait_minutes'].describe(percentiles=[.25, .5, .75, .9, .95, .99]))\n",
    "\n",
    "    print(\"\\n--- 总停留时长 (分钟) 的描述性统计 ---\")\n",
    "    print(char_que_df['stay_duration_minutes'].describe(percentiles=[.25, .5, .75, .9, .95, .99]))\n",
    "\n",
    "else:\n",
    "    print(\"错误: 'char_que_df' 未找到或为空。请先运行队列模拟部分的单元格。\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- 可视化: 停留与等待时间的分布直方图 ---\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"--- 正在生成停留与等待时间的分布直方图 ---\")\n",
    "\n",
    "if 'char_que_df' in locals() and 'wait_minutes' in char_que_df.columns:\n",
    "    # 设置绘图风格\n",
    "    sns.set_style(\"whitegrid\")\n",
    "\n",
    "    # 创建一个 1x2 的子图布局\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    fig.suptitle('Distribution of Waiting and Staying Durations', fontsize=16)\n",
    "\n",
    "    # a) 排队时间直方图\n",
    "    # 为了使图形更有意义，我们只关注那些实际需要排队的事件 (wait_minutes > 0)\n",
    "    waiting_events_df = char_que_df[char_que_df['wait_minutes'] > 0.1] # 使用0.1避免浮点数精度问题\n",
    "    sns.histplot(data=waiting_events_df, x='wait_minutes', bins=50, ax=axes[0], kde=True)\n",
    "    axes[0].set_title('Waiting Time Distribution (for events with wait > 0)')\n",
    "    axes[0].set_xlabel('Waiting Time (minutes)')\n",
    "    axes[0].set_ylabel('Number of Events')\n",
    "    # 限制 x 轴范围以便更好地观察主要分布，例如最多看 2 小时\n",
    "    axes[0].set_xlim(0, 120)\n",
    "\n",
    "    # b) 总停留时长直方图\n",
    "    sns.histplot(data=char_que_df, x='stay_duration_minutes', bins=50, ax=axes[1], color='skyblue', kde=True)\n",
    "    axes[1].set_title('Total Stay Duration Distribution')\n",
    "    axes[1].set_xlabel('Total Stay Duration (minutes)')\n",
    "    axes[1].set_ylabel('Number of Events')\n",
    "    # 限制 x 轴范围，例如最多看 3 小时\n",
    "    axes[1].set_xlim(0, 180)\n",
    "\n",
    "    # 调整布局以防止标题重叠并显示图像\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"错误: 'char_que_df' 或 'wait_minutes' 列未找到。请先运行前面的统计单元格。\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "# 1. 准备数据（都在 CPU）\n",
    "stations_pdf = pd.read_csv('station_information.csv')  # 充电站\n",
    "charging_pdf = charging_events.to_pandas()             # 充电行为（距离阈值内的停留点）\n",
    "\n",
    "# 2. 计算地图中心（深圳大致中心 or 所有点的平均）\n",
    "center_lat = charging_pdf['stay_lat'].mean()\n",
    "center_lon = charging_pdf['stay_lon'].mean()\n",
    "\n",
    "# 3. 创建底图\n",
    "m = folium.Map(location=[center_lat, center_lon],\n",
    "               zoom_start=11,\n",
    "               tiles='OpenStreetMap')\n",
    "\n",
    "# # 4. 充电站：用 MarkerCluster 以免过度遮挡\n",
    "# station_cluster = MarkerCluster(name='Charging Stations').add_to(m)\n",
    "# for _, row in stations_pdf.iterrows():\n",
    "#     folium.CircleMarker(\n",
    "#         location=[row['latitude'], row['longitude']],\n",
    "#         radius=4,\n",
    "#         color='blue',\n",
    "#         fill=True,\n",
    "#         fill_color='blue',\n",
    "#         fill_opacity=0.7,\n",
    "#         popup=f\"Station ID: {row['station_id']}\"\n",
    "#     ).add_to(station_cluster)\n",
    "\n",
    "# 5. 充电行为：红色半透明圆\n",
    "charge_cluster = MarkerCluster(name='Charging Events').add_to(m)\n",
    "for _, row in charging_pdf.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row['stay_lat'], row['stay_lon']],\n",
    "        radius=6,                      # 可改为 min(row['duration_s']/60, 10) 表示时长\n",
    "        color='red',\n",
    "        fill=True,\n",
    "        fill_color='red',\n",
    "        fill_opacity=0.5,\n",
    "        popup=(f\"TaxiID: {row['taxiid']}<br>\"\n",
    "               f\"Start: {row['start_time']}<br>\"\n",
    "               f\"End: {row['end_time']}<br>\"\n",
    "               f\"Duration: {row['duration_s']} s<br>\"\n",
    "               f\"Dist to Station: {row['distance_to_station_m']:.1f} m<br>\"\n",
    "               f\"Station ID: {row['nearest_station_id']}\")\n",
    "    ).add_to(charge_cluster)\n",
    "\n",
    "# 6. 图例/图层控制\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "# 7. 显示\n",
    "m"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
